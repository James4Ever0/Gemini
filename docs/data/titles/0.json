{
    "/README.md": "Multimodal Transformer for Text/Image Gen",
    "/README.md:1-11": "Gemini: Surpassing ChatGPT with Multi-Modal Transformer",
    "/README.md:11-44": "Efficient Gemini Transformer Model",
    "/README.md:131-174": "Unified Embeddings for Media Types",
    "/README.md:175-203": "Initialize AudioToEmbeddings Model",
    "/README.md:203-217": "Multimodal Gemini: Unlocking Insights Across Images, Audio, and Video",
    "/README.md:218-232": "Chain-of-Thought Prompting with Gemini Ultra",
    "/README.md:45-87": "Gemini Model Initialization and Input Application",
    "/README.md:88-129": "Multimodal SentencePiece Tokenizer Demo",
    "/example.py": "Efficient Model Reduction for Faster Processing",
    "/example.py:1-36": "Compact Gemini Model for Faster Processing",
    "/example.py:37-37": "Prints Variable 'y'",
    "/gemini_torch/__init__.py": "Importing GeminiTorch Components",
    "/gemini_torch/model.py": "Gemini Transformer Model: Multilayered, Versatile",
    "/gemini_torch/model.py:1-35": "Gemini Model: Transformer-based PyTorch Class",
    "/gemini_torch/model.py:104-140": "Forward Pass Multimodal Model",
    "/gemini_torch/model.py:141-151": "Torch Model: Code Concatenates and Decodes",
    "/gemini_torch/model.py:36-75": "Customizable Transformer Model Initialization",
    "/gemini_torch/model.py:76-103": "Gemini Model Initialization",
    "/gemini_torch/tokenizer.py": "Multimodal SentencePiece Tokenizer Builder",
    "/gemini_torch/tokenizer.py:1-30": "Multimodal SentencePiece Tokenizer Class",
    "/gemini_torch/tokenizer.py:118-150": "Modality Tokenizer: Encode-Decode Strings",
    "/gemini_torch/tokenizer.py:32-55": "Multimodal SentencePiece Tokenizer Initialization",
    "/gemini_torch/tokenizer.py:56-87": "Download and Define SentencePiece Tokenizer Class",
    "/gemini_torch/tokenizer.py:89-117": "Tokenizer Initialization and Encoding",
    "/gemini_torch/transformer.py": "Customizable Transformer with Memory Support",
    "/gemini_torch/transformer.py:1-51": "Transformer Layer Intermediates",
    "/gemini_torch/transformer.py:1001-1042": "Transformer Attention Layer Class",
    "/gemini_torch/transformer.py:1043-1076": "Initializing Transformer Layer in PyTorch",
    "/gemini_torch/transformer.py:107-151": "Transformer Loss Calculation Functions",
    "/gemini_torch/transformer.py:1078-1106": "Transformer Model Initialization",
    "/gemini_torch/transformer.py:1108-1134": "Position Biases in Flash Attention",
    "/gemini_torch/transformer.py:1135-1160": "Dynamic Positional Bias Init: Transformer Model Compatibility",
    "/gemini_torch/transformer.py:1162-1191": "Validating and Initializing Attributes for Transformer Layers",
    "/gemini_torch/transformer.py:1192-1224": "Setup Transformer Model Layer Order and Parameters",
    "/gemini_torch/transformer.py:1225-1249": "Transformer Layer Types Definition",
    "/gemini_torch/transformer.py:1251-1280": "Structured Dropout Transformer Model Construction",
    "/gemini_torch/transformer.py:1281-1309": "Configurable Transformer Model Initialization",
    "/gemini_torch/transformer.py:1310-1342": "Gemini Transformer Layer Initialization",
    "/gemini_torch/transformer.py:1343-1374": "Transformer Layer Operations",
    "/gemini_torch/transformer.py:1375-1405": "Layer Types in Transformer Model Processing",
    "/gemini_torch/transformer.py:1407-1445": "Vision Transformer Classes: Encoder, Decoder, CrossAttender",
    "/gemini_torch/transformer.py:1446-1479": "Transformer Model Initialization",
    "/gemini_torch/transformer.py:1480-1521": "Image Transformer for PyTorch",
    "/gemini_torch/transformer.py:152-191": "DeepNorm Transformer Initialization",
    "/gemini_torch/transformer.py:1522-1550": "Class Object Initialization and Embedding Setup",
    "/gemini_torch/transformer.py:1552-1577": "Transformer Model with Embedding Normalization",
    "/gemini_torch/transformer.py:1578-1615": "Transformer Model Forward Pass Function",
    "/gemini_torch/transformer.py:1617-1648": "Transformer Preprocessing Techniques",
    "/gemini_torch/transformer.py:1650-1678": "Masked Transformer Output Generator",
    "/gemini_torch/transformer.py:1679-1704": "Attention Transformer Processor",
    "/gemini_torch/transformer.py:193-229": "Dropout Sequence Initialization",
    "/gemini_torch/transformer.py:232-266": "Transformer Layer Components",
    "/gemini_torch/transformer.py:268-299": "Scaled Sinusoidal Positional Embeddings",
    "/gemini_torch/transformer.py:300-333": "Position-Aware Embedding Layer Initialization",
    "/gemini_torch/transformer.py:335-364": "Dynamic Position Bias for Transformers",
    "/gemini_torch/transformer.py:366-401": "MLP Position Embeddings for Transformers",
    "/gemini_torch/transformer.py:403-432": "Positional Bias Calculator for Transformer Models",
    "/gemini_torch/transformer.py:433-468": "Logarithmic Biases for Head Counts",
    "/gemini_torch/transformer.py:469-500": "Rotary Embedding Initialization",
    "/gemini_torch/transformer.py:502-540": "Rotary Position Embedding for Scale",
    "/gemini_torch/transformer.py:53-106": "Tensor Manipulation Functions in Gemini-Torch",
    "/gemini_torch/transformer.py:541-584": "Custom Normalization Layers for PyTorch",
    "/gemini_torch/transformer.py:587-617": "GRU Residual Gating in Transformers",
    "/gemini_torch/transformer.py:618-661": "ShiftTokens with GLU for FeedForward",
    "/gemini_torch/transformer.py:662-700": "FeedForward Initialization with Options",
    "/gemini_torch/transformer.py:701-744": "Self-Attention and Feed-Forward Layers",
    "/gemini_torch/transformer.py:745-776": "Initialize Transformer Layer with Configurable Params",
    "/gemini_torch/transformer.py:778-802": "Initializing Transformer Model Layers",
    "/gemini_torch/transformer.py:803-827": "Initializing Transformer's Attention Module",
    "/gemini_torch/transformer.py:828-863": "Advanced Transformer Model Implementation",
    "/gemini_torch/transformer.py:864-901": "Transformer Input Processing",
    "/gemini_torch/transformer.py:902-930": "Rotary Position Embedded Linear Attention",
    "/gemini_torch/transformer.py:931-964": "Normalizing and Masking Attention in Transformer",
    "/gemini_torch/transformer.py:965-999": "Multi-Head Masked Self-Attention",
    "/gemini_torch/utils.py": "Image and Audio Embeddings Generator",
    "/gemini_torch/utils.py:1-32": "Patch-based Image to Text Embeddings",
    "/gemini_torch/utils.py:33-52": "Patch-based Image Tokenization",
    "/gemini_torch/utils.py:54-88": "AudioToEmbeddings: Projecting Audio to 512-Dim Embeddings",
    "/gemini_torch/utils.py:90-108": "Forward Pass Projection",
    "/pyproject.toml": "PyProject Configuration for Gemini-Torch",
    "/pyproject.toml:1-39": "PyProject Configuration for Gemini-Torch with Poetry",
    "/pyproject.toml:40-54": "Python Project Config: Pypackages and Linting",
    "/requirements.txt": "Python Packages for Data-Intensive ML Projects",
    "/tests/test_audio_embedder.py": "Testing Audio Embedder Function Compatibility",
    "/tests/test_audio_embedder.py:1-32": "Test AudioToEmbeddings Model Shape",
    "/tests/test_audio_embedder.py:123-147": "Output Shape Testing",
    "/tests/test_audio_embedder.py:33-62": "Testing Audio Embedder Edge Cases",
    "/tests/test_audio_embedder.py:63-91": "Test Audio Embedding Model Functions",
    "/tests/test_audio_embedder.py:92-122": "Audio Embedder Shape Testing",
    "/tests/test_gemini.py": "Comprehensive Gemini Model Testing with Pytest",
    "/tests/test_gemini.py:1-34": "Gemini Model Tests in Gemini_Torch",
    "/tests/test_gemini.py:125-133": "Testing Gemini Model Copies and Error Handling",
    "/tests/test_gemini.py:37-66": "Gemini Module Test: Forward Pass Validation",
    "/tests/test_gemini.py:67-96": "Gemini Model Test Functions",
    "/tests/test_gemini.py:97-122": "Gemini Model Test Suite",
    "/tests/test_img_encoder.py": "Testing Image Encoder Initialization and Output Shape",
    "/tests/test_img_encoder.py:1-34": "ImageToTextEmbeddings Model Unit Tests",
    "/tests/test_img_encoder.py:35-46": "Forward Pass Image Resizing Test",
    "/tests/test_img_to_transformer.py": "Image-to-Text Embeddings in Gemini_torch: Demonstration and Implementation",
    "/tests/test_tokenizer.py": "Multimodal SentencePiece Tokenizer Tests",
    "/tests/test_tokenizer.py:1-37": "Testing Multimodal SentencePiece Tokenizer Decoding",
    "/tests/test_tokenizer.py:102-127": "Multimodal SentencePiece Tokenizer Tests",
    "/tests/test_tokenizer.py:128-151": "Tokenizer Functionality Test",
    "/tests/test_tokenizer.py:152-172": "Tokenizer Encoding Test",
    "/tests/test_tokenizer.py:174-197": "Tokenizer Decode Testing",
    "/tests/test_tokenizer.py:199-221": "Tokenizer Consistency Tests",
    "/tests/test_tokenizer.py:223-242": "Tokenizer Test Suite",
    "/tests/test_tokenizer.py:244-246": "Tokenizer Equality Check",
    "/tests/test_tokenizer.py:38-71": "Testing Tokenizer Edge Cases",
    "/tests/test_tokenizer.py:72-101": "Encoding Edge Cases Test"
}