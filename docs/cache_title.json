{"_default": {"1": {"path": "/README.md", "hash": "97ef6e6b8f14469b08ff43120753d566", "title": "Multimodal Transformer for Text/Image Gen"}, "2": {"path": "/README.md:1-11", "hash": "e1d6785e66ab3a6592db1bf0c5d5c42d", "title": "Gemini: Surpassing ChatGPT with Multi-Modal Transformer"}, "3": {"path": "/README.md:11-44", "hash": "ecfd8a56547d9fdbdb2154822ec10ee1", "title": "Efficient Gemini Transformer Model"}, "4": {"path": "/README.md:45-87", "hash": "a711ef3369edabb5e817613f41a29ddc", "title": "Gemini Model Initialization and Input Application"}, "5": {"path": "/README.md:88-129", "hash": "830d4813e4acbf5ecf702a0fa71d8606", "title": "Multimodal SentencePiece Tokenizer Demo"}, "6": {"path": "/README.md:131-174", "hash": "bf2f8b42489740f47280a1923ccd765d", "title": "Unified Embeddings for Media Types"}, "7": {"path": "/README.md:175-203", "hash": "f653ac9d40573672000eb9ab4ed18940", "title": "Initialize AudioToEmbeddings Model"}, "8": {"path": "/README.md:203-217", "hash": "f1d61cc9b3c03b45cb008ef3c236e3bb", "title": "Multimodal Gemini: Unlocking Insights Across Images, Audio, and Video"}, "9": {"path": "/README.md:218-232", "hash": "dd58cf30630321d3e3ce12adf1cc9f0c", "title": "Chain-of-Thought Prompting with Gemini Ultra"}, "10": {"path": "/example.py", "hash": "87fc36a153b254a6c0d1691a7483e43a", "title": "Efficient Model Reduction for Faster Processing"}, "11": {"path": "/example.py:1-36", "hash": "72b132fab604bd783cd617b6b6bed0fe", "title": "Compact Gemini Model for Faster Processing"}, "12": {"path": "/example.py:37-37", "hash": "af3f49f71555fdaa02f2c51e2bafc092", "title": "Prints Variable 'y'"}, "13": {"path": "/gemini_torch/__init__.py", "hash": "ec78164cc939bb4cfd03b31c51b1bb63", "title": "Importing GeminiTorch Components"}, "14": {"path": "/gemini_torch/model.py", "hash": "c3e4571b1edd84f021d1ea3eb3764a10", "title": "Gemini Transformer Model: Multilayered, Versatile"}, "15": {"path": "/gemini_torch/model.py:1-35", "hash": "a839c0d57254de255634b3661f69f8e1", "title": "Gemini Model: Transformer-based PyTorch Class"}, "16": {"path": "/gemini_torch/model.py:36-75", "hash": "eadcdcfb5b039b7d65fcc21952b97599", "title": "Customizable Transformer Model Initialization"}, "17": {"path": "/gemini_torch/model.py:76-103", "hash": "9eabad9ecaf38634399b50ace8038f11", "title": "Gemini Model Initialization"}, "18": {"path": "/gemini_torch/model.py:104-140", "hash": "fe1938b53d0db5e872f946fb7c01b4dd", "title": "Forward Pass Multimodal Model"}, "19": {"path": "/gemini_torch/model.py:141-151", "hash": "d9f2712360c545656fcec63dfaeefbde", "title": "Torch Model: Code Concatenates and Decodes"}, "20": {"path": "/gemini_torch/tokenizer.py", "hash": "fdb62a78263773f1dce711ab9202a558", "title": "Multimodal SentencePiece Tokenizer Builder"}, "21": {"path": "/gemini_torch/tokenizer.py:1-30", "hash": "78643e6ccea915ef0980ef958c1a35fa", "title": "Multimodal SentencePiece Tokenizer Class"}, "22": {"path": "/gemini_torch/tokenizer.py:32-55", "hash": "212ff1c0c95f67769441a8883f31b9f3", "title": "Multimodal SentencePiece Tokenizer Initialization"}, "23": {"path": "/gemini_torch/tokenizer.py:56-87", "hash": "0dd21c4225cdd5db001a07f0c943b2d8", "title": "Download and Define SentencePiece Tokenizer Class"}, "24": {"path": "/gemini_torch/tokenizer.py:89-117", "hash": "d2c0f9195e807fa5d6019f4b186e56f5", "title": "Tokenizer Initialization and Encoding"}, "25": {"path": "/gemini_torch/tokenizer.py:118-150", "hash": "9067db5bba0b6bc20a0e7fd01aad9992", "title": "Modality Tokenizer: Encode-Decode Strings"}, "26": {"path": "/gemini_torch/transformer.py", "hash": "5b8a05899199743e32d2fa4d6d611a13", "title": "Customizable Transformer with Memory Support"}, "27": {"path": "/gemini_torch/transformer.py:1-51", "hash": "669efd4f116550671bebae57a1b645a3", "title": "Transformer Layer Intermediates"}, "28": {"path": "/gemini_torch/transformer.py:53-106", "hash": "6c9cba8bc5a88d865878068be0e533ad", "title": "Tensor Manipulation Functions in Gemini-Torch"}, "29": {"path": "/gemini_torch/transformer.py:107-151", "hash": "07e64eae1d6129f9d34dc418f6b87d60", "title": "Transformer Loss Calculation Functions"}, "30": {"path": "/gemini_torch/transformer.py:152-191", "hash": "4435f07da42a961c4337a3d849c5a57b", "title": "DeepNorm Transformer Initialization"}, "31": {"path": "/gemini_torch/transformer.py:193-229", "hash": "0d438069eb4f6e2b530fe75aaffcf520", "title": "Dropout Sequence Initialization"}, "32": {"path": "/gemini_torch/transformer.py:232-266", "hash": "7436e7d7380c1867e51c1aea0c5304e8", "title": "Transformer Layer Components"}, "33": {"path": "/gemini_torch/transformer.py:268-299", "hash": "9a3f15fc1a654d94d4ceb2500decf0fd", "title": "Scaled Sinusoidal Positional Embeddings"}, "34": {"path": "/gemini_torch/transformer.py:300-333", "hash": "607472c630ab4f5ff1edc307d46896a4", "title": "Position-Aware Embedding Layer Initialization"}, "35": {"path": "/gemini_torch/transformer.py:335-364", "hash": "e6b4b543f0bd72c1699cf4b3adbb1d44", "title": "Dynamic Position Bias for Transformers"}, "36": {"path": "/gemini_torch/transformer.py:366-401", "hash": "881049a4c6954bdcfeeb48d0bab3dcce", "title": "MLP Position Embeddings for Transformers"}, "37": {"path": "/gemini_torch/transformer.py:403-432", "hash": "619f8db69a8991ac275a29b867531cfe", "title": "Positional Bias Calculator for Transformer Models"}, "38": {"path": "/gemini_torch/transformer.py:433-468", "hash": "a70b42ef20f81a88b120a9080ac2edb8", "title": "Logarithmic Biases for Head Counts"}, "39": {"path": "/gemini_torch/transformer.py:469-500", "hash": "dd41fc9ad29b4a80c5cc3ab31fc25500", "title": "Rotary Embedding Initialization"}, "40": {"path": "/gemini_torch/transformer.py:502-540", "hash": "e438ded20ac99fafe81c28eb37b2ae40", "title": "Rotary Position Embedding for Scale"}, "41": {"path": "/gemini_torch/transformer.py:541-584", "hash": "0a76354d086fc00c1d4061341e65083e", "title": "Custom Normalization Layers for PyTorch"}, "42": {"path": "/gemini_torch/transformer.py:587-617", "hash": "201987b3f2bbd147813814f3468431ef", "title": "GRU Residual Gating in Transformers"}, "43": {"path": "/gemini_torch/transformer.py:618-661", "hash": "8008ccc437e8356f010b6c28b8d8c3e7", "title": "ShiftTokens with GLU for FeedForward"}, "44": {"path": "/gemini_torch/transformer.py:662-700", "hash": "abf3d28b5e7b7d02152089c8542b8100", "title": "FeedForward Initialization with Options"}, "45": {"path": "/gemini_torch/transformer.py:701-744", "hash": "9089425910bdec331979ac4111e5ad4a", "title": "Self-Attention and Feed-Forward Layers"}, "46": {"path": "/gemini_torch/transformer.py:745-776", "hash": "6723c34f7d1cc5302af6cf283d4b34bd", "title": "Initialize Transformer Layer with Configurable Params"}, "47": {"path": "/gemini_torch/transformer.py:778-802", "hash": "8ab8e2c8812e1af0b0e22cf1e7eb5e47", "title": "Initializing Transformer Model Layers"}, "48": {"path": "/gemini_torch/transformer.py:803-827", "hash": "a5747606a2769873dbdbe3dfa444a20e", "title": "Initializing Transformer's Attention Module"}, "49": {"path": "/gemini_torch/transformer.py:828-863", "hash": "8039726b2565c0e5d51f9cea54ebe958", "title": "Advanced Transformer Model Implementation"}, "50": {"path": "/gemini_torch/transformer.py:864-901", "hash": "4bbc36f4c172026b4e6555a7c457501c", "title": "Transformer Input Processing"}, "51": {"path": "/gemini_torch/transformer.py:902-930", "hash": "d4395402ec167078c768393a820129f4", "title": "Rotary Position Embedded Linear Attention"}, "52": {"path": "/gemini_torch/transformer.py:931-964", "hash": "ce7cc408413bd416229a396f43def8cb", "title": "Normalizing and Masking Attention in Transformer"}, "53": {"path": "/gemini_torch/transformer.py:965-999", "hash": "a8aa01fc571106489a2a88f59acdf496", "title": "Multi-Head Masked Self-Attention"}, "54": {"path": "/gemini_torch/transformer.py:1001-1042", "hash": "4848cfdc98772e2ca764217f3cb7116f", "title": "Transformer Attention Layer Class"}, "55": {"path": "/gemini_torch/transformer.py:1043-1076", "hash": "8068f73a7a62af658aac72392d8e3bd9", "title": "Initializing Transformer Layer in PyTorch"}, "56": {"path": "/gemini_torch/transformer.py:1078-1106", "hash": "3e4076e9841ef079f848e3ddc67a5c9c", "title": "Transformer Model Initialization"}, "57": {"path": "/gemini_torch/transformer.py:1108-1134", "hash": "b8f09faa481f8bf18fc5fab92c625a12", "title": "Position Biases in Flash Attention"}, "58": {"path": "/gemini_torch/transformer.py:1135-1160", "hash": "36e36ec393c32a0ab1879daf41f587bf", "title": "Dynamic Positional Bias Init: Transformer Model Compatibility"}, "59": {"path": "/gemini_torch/transformer.py:1162-1191", "hash": "b9286baa83fb203310ebf1e48a8af37a", "title": "Validating and Initializing Attributes for Transformer Layers"}, "60": {"path": "/gemini_torch/transformer.py:1192-1224", "hash": "c0e9d55d8697dc2197c82af858772cdb", "title": "Setup Transformer Model Layer Order and Parameters"}, "61": {"path": "/gemini_torch/transformer.py:1225-1249", "hash": "50a73927a4f9e85c682f6af7053d3e1d", "title": "Transformer Layer Types Definition"}, "62": {"path": "/gemini_torch/transformer.py:1251-1280", "hash": "51bc2b42da950440a0cb5ec92bdf6db9", "title": "Structured Dropout Transformer Model Construction"}, "63": {"path": "/gemini_torch/transformer.py:1281-1309", "hash": "784262b3551712ffea2e8f37faff76c1", "title": "Configurable Transformer Model Initialization"}, "64": {"path": "/gemini_torch/transformer.py:1310-1342", "hash": "729ba3ec173077cf839e711624130ed3", "title": "Gemini Transformer Layer Initialization"}, "65": {"path": "/gemini_torch/transformer.py:1343-1374", "hash": "4f086b5442ccc9df9f40bd6d779c9c54", "title": "Transformer Layer Operations"}, "66": {"path": "/gemini_torch/transformer.py:1375-1405", "hash": "1a75c576d673ca3635259a33377e15e5", "title": "Layer Types in Transformer Model Processing"}, "67": {"path": "/gemini_torch/transformer.py:1407-1445", "hash": "6f8daa453cfd7871f5b0929cc5fddb21", "title": "Vision Transformer Classes: Encoder, Decoder, CrossAttender"}, "68": {"path": "/gemini_torch/transformer.py:1446-1479", "hash": "c3441ce72a83e2de86a1e3d4bf9821bb", "title": "Transformer Model Initialization"}, "69": {"path": "/gemini_torch/transformer.py:1480-1521", "hash": "9bdc7c49ce4ea5c72b2c8d92cf12153a", "title": "Image Transformer for PyTorch"}, "70": {"path": "/gemini_torch/transformer.py:1522-1550", "hash": "eb854609e8eb3538e641ecfb3f48d0fa", "title": "Class Object Initialization and Embedding Setup"}, "71": {"path": "/gemini_torch/transformer.py:1552-1577", "hash": "6469707a0cf61ba19b5cf8dc26fcf69f", "title": "Transformer Model with Embedding Normalization"}, "72": {"path": "/gemini_torch/transformer.py:1578-1615", "hash": "efd7cac6d7d78c16a8a2541fed760c98", "title": "Transformer Model Forward Pass Function"}, "73": {"path": "/gemini_torch/transformer.py:1617-1648", "hash": "d1f0402fe52683cfe46467748c07174a", "title": "Transformer Preprocessing Techniques"}, "74": {"path": "/gemini_torch/transformer.py:1650-1678", "hash": "7a8e7b020d7c206794ae58b3cefede3b", "title": "Masked Transformer Output Generator"}, "75": {"path": "/gemini_torch/transformer.py:1679-1704", "hash": "423f56ba63c9df994e42eda56956287d", "title": "Attention Transformer Processor"}, "76": {"path": "/gemini_torch/utils.py", "hash": "c61913eea1964f904e59ff228bd48574", "title": "Image and Audio Embeddings Generator"}, "77": {"path": "/gemini_torch/utils.py:1-32", "hash": "970542ec45af968f0e277343ab2b86bc", "title": "Patch-based Image to Text Embeddings"}, "78": {"path": "/gemini_torch/utils.py:33-52", "hash": "1ff7d8a50bd2ed0869931095715007f0", "title": "Patch-based Image Tokenization"}, "79": {"path": "/gemini_torch/utils.py:54-88", "hash": "fe95e31a4ea8dac4666482bd9f19c3c0", "title": "AudioToEmbeddings: Projecting Audio to 512-Dim Embeddings"}, "80": {"path": "/gemini_torch/utils.py:90-108", "hash": "7b3521489d075322032014a8f4307554", "title": "Forward Pass Projection"}, "81": {"path": "/pyproject.toml", "hash": "5c9b1266763779c0b45ab271207c73aa", "title": "PyProject Configuration for Gemini-Torch"}, "82": {"path": "/pyproject.toml:1-39", "hash": "d7f128f410fe8230f3e6ecc72f437591", "title": "PyProject Configuration for Gemini-Torch with Poetry"}, "83": {"path": "/pyproject.toml:40-54", "hash": "bbce538e2c2975737545ef797df445f9", "title": "Python Project Config: Pypackages and Linting"}, "84": {"path": "/requirements.txt", "hash": "37e745c881df811a9dd6b089bdb54683", "title": "Python Packages for Data-Intensive ML Projects"}, "85": {"path": "/tests/test_audio_embedder.py", "hash": "eeae5e02147b140a058369394bb32e9a", "title": "Testing Audio Embedder Function Compatibility"}, "86": {"path": "/tests/test_audio_embedder.py:1-32", "hash": "8f8a7be9c25a7c008bdd8042366e73db", "title": "Test AudioToEmbeddings Model Shape"}, "87": {"path": "/tests/test_audio_embedder.py:33-62", "hash": "cb4e4eaa57b60461dcf894263916fc02", "title": "Testing Audio Embedder Edge Cases"}, "88": {"path": "/tests/test_audio_embedder.py:63-91", "hash": "add179ab20bce309aff390f8f088af59", "title": "Test Audio Embedding Model Functions"}, "89": {"path": "/tests/test_audio_embedder.py:92-122", "hash": "745370b70d727897a801e1bb9e6844c0", "title": "Audio Embedder Shape Testing"}, "90": {"path": "/tests/test_audio_embedder.py:123-147", "hash": "8542df9966ea45a9a1274505586fb30d", "title": "Output Shape Testing"}, "91": {"path": "/tests/test_gemini.py", "hash": "4a2ba12286cb86429e97aab707b25e84", "title": "Comprehensive Gemini Model Testing with Pytest"}, "92": {"path": "/tests/test_gemini.py:1-34", "hash": "4e9ff8b379c20c7f10ae5bb273c4bca5", "title": "Gemini Model Tests in Gemini_Torch"}, "93": {"path": "/tests/test_gemini.py:37-66", "hash": "1c7db683bdee3b2fb2515803b414f546", "title": "Gemini Module Test: Forward Pass Validation"}, "94": {"path": "/tests/test_gemini.py:67-96", "hash": "69499dd8b67cff4f32cf839deb985ee2", "title": "Gemini Model Test Functions"}, "95": {"path": "/tests/test_gemini.py:97-122", "hash": "75762977b5d6ef9c060892e2d81574ee", "title": "Gemini Model Test Suite"}, "96": {"path": "/tests/test_gemini.py:125-133", "hash": "fc24d133baf2947f6dde21fbf843444b", "title": "Testing Gemini Model Copies and Error Handling"}, "97": {"path": "/tests/test_img_encoder.py", "hash": "56e337eca59c72f501117948800bd7cf", "title": "Testing Image Encoder Initialization and Output Shape"}, "98": {"path": "/tests/test_img_encoder.py:1-34", "hash": "bdf5af726df1b0852c8ce0f839d3e7e8", "title": "ImageToTextEmbeddings Model Unit Tests"}, "99": {"path": "/tests/test_img_encoder.py:35-46", "hash": "7edf25a8ddecca2c8c7d0aed9ba4f164", "title": "Forward Pass Image Resizing Test"}, "100": {"path": "/tests/test_img_to_transformer.py", "hash": "e18f04d0f3cd88c6475554fcffb7d127", "title": "Image-to-Text Embeddings in Gemini_torch: Demonstration and Implementation"}, "101": {"path": "/tests/test_tokenizer.py", "hash": "56d4a8e0800e65360ab3a27bdb04e938", "title": "Multimodal SentencePiece Tokenizer Tests"}, "102": {"path": "/tests/test_tokenizer.py:1-37", "hash": "5538daba353f213dd5a4225724e32edc", "title": "Testing Multimodal SentencePiece Tokenizer Decoding"}, "103": {"path": "/tests/test_tokenizer.py:38-71", "hash": "7d371d6e70386eddc75a5c05bc0973f8", "title": "Testing Tokenizer Edge Cases"}, "104": {"path": "/tests/test_tokenizer.py:72-101", "hash": "9a446ef81c64077286edfce6c3db5d47", "title": "Encoding Edge Cases Test"}, "105": {"path": "/tests/test_tokenizer.py:102-127", "hash": "2b85e2bcf5694c0a308f6454251d0431", "title": "Multimodal SentencePiece Tokenizer Tests"}, "106": {"path": "/tests/test_tokenizer.py:128-151", "hash": "716c85989d664f8afb902b14476e3d9c", "title": "Tokenizer Functionality Test"}, "107": {"path": "/tests/test_tokenizer.py:152-172", "hash": "860f301f519f81b6a8bd80dfb27c2d5e", "title": "Tokenizer Encoding Test"}, "108": {"path": "/tests/test_tokenizer.py:174-197", "hash": "9dd8ee3666f1594d88333b14c28da239", "title": "Tokenizer Decode Testing"}, "109": {"path": "/tests/test_tokenizer.py:199-221", "hash": "0822a40492a04b0894211356f8ecbe3e", "title": "Tokenizer Consistency Tests"}, "110": {"path": "/tests/test_tokenizer.py:223-242", "hash": "fa44f5af1a9547aac1035d38113379a0", "title": "Tokenizer Test Suite"}, "111": {"path": "/tests/test_tokenizer.py:244-246", "hash": "c3de77b2d3f40db8e8a33327c4d8ae16", "title": "Tokenizer Equality Check"}}}