{
    "summary": "The ImageToTextEmbeddings class transforms images into text tokens using patch-based embedding, while the AudioToEmbeddings class is a neural network module that takes in an audio sequence and outputs embeddings. Both classes process input data, generate tokens, and match desired sequence length and dimension.",
    "details": [
        {
            "comment": "The code defines a class `ImageToTextEmbeddings` which converts images into text tokens using patch-based embedding. It takes the image size, dimension for each patch, and desired sequence length as inputs. The forward method processes the input images by applying a projection layer and a sequential projection layer to create text tokens.",
            "location": "\"/media/root/Toshiba XG3/works/Gemini/docs/src/gemini_torch/utils.py\":0-31",
            "content": "from einops import rearrange, reduce\nfrom torch import nn\nclass ImageToTextEmbeddings(nn.Module):\n    \"\"\"\n    Converts images into text tokens using patch-based embedding.\n    Args:\n        patch_size (int): The size of each patch in the image.\n        dim (int): The dimension of the embedding for each patch.\n        seq_len (int): The desired sequence length of the text tokens.\n    Returns:\n        torch.Tensor: The text tokens representing the input images.\n    \"\"\"\n    def __init__(self, patch_size, dim, seq_len):\n        super().__init__()\n        self.patch_size = patch_size\n        self.dim = dim\n        self.seq_len = seq_len\n        self.projection = nn.Linear(patch_size * patch_size * 3, dim)\n        # self.seq_proj = nn.Linear(dim, seq_len)\n    def forward(self, images):\n        # Input images are assumed to be in the shape (batch_size, channels, height, width)\n        batch_size, _, height, width = images.shape\n        seq_proj = nn.Linear(height, self.seq_len)\n        # Ensure that the image dimensions are divisible by the patch size"
        },
        {
            "comment": "This code segment receives images and processes them to create text tokens. It first checks if image dimensions are divisible by the patch size, then rearranges the images into patches using einops. The patches are projected into embedding dimension, reshaped, and reduced to obtain text tokens. Finally, these tokens are projected again to match the desired sequence length before being returned.",
            "location": "\"/media/root/Toshiba XG3/works/Gemini/docs/src/gemini_torch/utils.py\":32-51",
            "content": "        assert height % self.patch_size == 0 and width % self.patch_size == 0, \\\n            \"Image dimensions must be divisible by the patch size\"\n        # Rearrange the images into patches using einops\n        patches = rearrange(images, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=self.patch_size, p2=self.patch_size)\n        # Project the patches into the embedding dimension\n        embeddings = self.projection(patches)\n        # Reshape the embeddings into the shape (batch_size, seq_len, dim)\n        seq_len = (height // self.patch_size) * (width // self.patch_size)\n        text_tokens = rearrange(embeddings, 'b (h w) e -> b h w e', h=seq_len, w=1)\n        text_tokens = reduce(text_tokens, \"b h w e -> b h (w e)\", \"mean\")\n        # Project the embeddings into the sequence length, in the 2nd dimension\n        text_tokens = rearrange(text_tokens, \"b h d -> b d h\", h=seq_len)\n        text_tokens = seq_proj(text_tokens)\n        text_tokens = rearrange(text_tokens, \"b d h -> b h d\")\n        return text_tokens"
        },
        {
            "comment": "The AudioToEmbeddings class is a neural network module that takes in an audio sequence and outputs embeddings. It initializes a linear layer to project the 2D audio input to the desired 3D shape, with the number of dimensions (dim) being 512. The class parameters include audio_seq_len for the length of the audio sequence, seqlen for the length of the output sequence, and dim for the dimension of the embedding.",
            "location": "\"/media/root/Toshiba XG3/works/Gemini/docs/src/gemini_torch/utils.py\":53-87",
            "content": "# x = torch.randn(1, 3, 64, 64)\n# model = ImageToTextEmbeddings(patch_size=8, dim=512, seq_len=128)\n# y = model(x)\n# print(y.shape)  # Should be [1, 64, 512]\nclass AudioToEmbeddings(nn.Module):\n    \"\"\"AudioToEmbeddings\n    Args:\n        audio_seq_len (int): Length of the audio sequence\n        seqlen (int): Length of the sequence\n        dim (int): Embedding dimension\n    Example:\n        >>> import torch\n        >>> from geminix import AudioToEmbeddings\n        >>> model = AudioToEmbeddings(\n        ...     audio_seq_len=32000,\n        ...     seqlen=512,\n        ...     dim=512\n        ... )\n        >>> x = torch.randn(1, 32000)\n        >>> y = model(x)\n        >>> y.shape\n        torch.Size([1, 512, 512])\n    \"\"\"\n    def __init__(self, audio_seq_len: int, seqlen: int, dim: int):\n        super(AudioToEmbeddings, self).__init__()\n        self.audio_seq_len = audio_seq_len\n        self.seqlen = seqlen\n        self.dim = dim\n        # Initialize a linear layer to project the 2D audio input to the desired 3D shape\n        self.projection = nn.Linear(audio_seq_len, seqlen * dim)"
        },
        {
            "comment": "This function performs a forward pass on the input tensor `x`. It first projects the audio tensor to match the target sequence length and dimension, then reshapes it into the desired shape. The resulting tensor is of size [batch, seqlen, dim].",
            "location": "\"/media/root/Toshiba XG3/works/Gemini/docs/src/gemini_torch/utils.py\":89-107",
            "content": "    def forward(self, x):\n        \"\"\"Forward pass\n        Args:\n            x (_type_): _description_\n        Returns:\n            _type_: _description_\n        \"\"\"\n        # x shape: [batch, audio_seq_len] - 2D input\n        batch, audio_seq_len = x.shape\n        # Project the audio tensor to match the seqlen and dim\n        x = self.projection(x)  # x shape: [batch, seqlen * dim]\n        # Reshape to the target shape: [batch, seqlen, dim]\n        x = rearrange(x, \"b (s d) -> b s d\", s=self.seqlen, d=self.dim)\n        return x"
        }
    ]
}