{
    "summary": "The code creates a Multimodal SentencePiece tokenizer for encoding and decoding strings into token IDs, with support for modality tokens, BOS, and EOS. It initializes by downloading the model and saves the vocabulary size.",
    "details": [
        {
            "comment": "This code defines a class for Multimodal SentencePiece tokenizer which can be used to encode and decode strings into lists of token IDs. It takes the path to the model file or tokenizer name as arguments. The PRETRAINED_VOCAB_FILES_MAP provides URLs for pre-trained models. It uses SentencePieceProcessor from the sentencepiece library.",
            "location": "\"/media/root/Toshiba XG3/works/Gemini/docs/src/gemini_torch/tokenizer.py\":0-29",
            "content": "import os\nimport requests\nfrom logging import getLogger\nfrom typing import List, Optional\nfrom sentencepiece import SentencePieceProcessor\nlogger = getLogger()\nPRETRAINED_VOCAB_FILES_MAP = {\n    \"vocab_file\": {\n        \"hf-internal-testing/llama-tokenizer\": \"https://huggingface.co/hf-internal-testing/llama-tokenizer/resolve/main/tokenizer.model\",\n    },\n    \"tokenizer_file\": {\n        \"hf-internal-testing/llama-tokenizer\": \"https://huggingface.co/hf-internal-testing/llama-tokenizer/resolve/main/tokenizer_config.json\",\n    },\n}\nclass MultimodalSentencePieceTokenizer:\n    \"\"\"Multimodal SentencePiece tokenizer.\n    Args:\n        model_path (str, optional): Path to the SentencePiece model file. Defaults to None.\n        tokenizer_name (str, optional): Name of the tokenizer to download. Defaults to None.\n    Methods:\n        encode(s: str, modality: str, bos: bool = True, eos: bool = True) -> List[int]: Encodes a string into a list of token IDs.\n        decode(tokens: List[int]) -> str: Decodes a list of token IDs into a string."
        },
        {
            "comment": "The code initializes a multimodal sentence piece tokenizer with the option to provide a model path or tokenizer name. It checks if the model_path exists and downloads the tokenizer if only tokenizer_name is provided. The SentencePieceProcessor is then loaded, and the number of words in the vocabulary is saved for future use.",
            "location": "\"/media/root/Toshiba XG3/works/Gemini/docs/src/gemini_torch/tokenizer.py\":31-54",
            "content": "    Examples:\n        >>> tokenizer_name = \"hf-internal-testing/llama-tokenizer\"\n        >>> tokenizer = MultimodalSentencePieceTokenizer(tokenizer_name=tokenizer_name)\n        >>> encoded_audio = tokenizer.encode(\"Audio description\", modality='audio')\n        >>> decoded_audio = tokenizer.decode(encoded_audio)\n        >>> print(\"Encoded audio:\", encoded_audio)\n        >>> print(\"Decoded audio:\", decoded_audio)\n    \"\"\"\n    def __init__(\n        self, model_path: Optional[str] = None, tokenizer_name: Optional[str] = None\n    ):\n        if model_path:\n            assert os.path.isfile(model_path), model_path\n        elif tokenizer_name:\n            model_path = self.download_tokenizer(tokenizer_name)\n        else:\n            raise ValueError(\"Either model_path or tokenizer_name must be provided.\")\n        self.sp_model = SentencePieceProcessor(model_file=model_path)\n        logger.info(f\"Reloaded SentencePiece model from {model_path}\")\n        # Initialize token IDs\n        self.n_words: int = self.sp_model.vocab_size()"
        },
        {
            "comment": "This code is defining a class for tokenizer, which includes methods to initialize special token IDs and download the SentencePiece model from HuggingFace Hub. It also provides special token IDs for image and audio modalities. The download_tokenizer method takes in a tokenizer name as argument and returns the path of the downloaded model file if available, or raises ValueError if the specified tokenizer is not available.",
            "location": "\"/media/root/Toshiba XG3/works/Gemini/docs/src/gemini_torch/tokenizer.py\":55-86",
            "content": "        self.bos_id: int = self.sp_model.bos_id()\n        self.eos_id: int = self.sp_model.eos_id()\n        self.pad_id: int = self.sp_model.pad_id()\n        # Initialize special token IDs for modalities\n        self.modality_tokens = {\n            \"image\": (\n                self.sp_model.piece_to_id(\"<img>\"),\n                self.sp_model.piece_to_id(\"</img>\"),\n            ),\n            \"audio\": (\n                self.sp_model.piece_to_id(\"<audio>\"),\n                self.sp_model.piece_to_id(\"</audio>\"),\n            ),\n        }\n    @staticmethod\n    def download_tokenizer(tokenizer_name: str) -> str:\n        \"\"\"Downloads the SentencePiece model file from HuggingFace Hub.\n        Args:\n            tokenizer_name (str): _description_\n        Raises:\n            ValueError: _description_\n            Exception: _description_\n        Returns:\n            str: _description_\n        \"\"\"\n        if tokenizer_name not in PRETRAINED_VOCAB_FILES_MAP[\"vocab_file\"]:\n            raise ValueError(f\"Tokenizer {tokenizer_name} is not available.\")"
        },
        {
            "comment": "This code initializes a tokenizer by downloading the SentencePiece model file from the specified URL and saving it to the \"data/tokenizer.model\" location. The encode function takes a string, modality, and optional boolean values for BOS (beginning of sequence) and EOS (end of sequence) tokens, and returns a list of token IDs representing the encoded input.",
            "location": "\"/media/root/Toshiba XG3/works/Gemini/docs/src/gemini_torch/tokenizer.py\":88-116",
            "content": "        model_url = PRETRAINED_VOCAB_FILES_MAP[\"vocab_file\"][tokenizer_name]\n        model_path = os.path.join(\"data\", \"tokenizer.model\")\n        if not os.path.exists(\"data\"):\n            os.makedirs(\"data\")\n        # Downloading the tokenizer model file\n        response = requests.get(model_url)\n        if response.status_code == 200:\n            with open(model_path, \"wb\") as file:\n                file.write(response.content)\n            logger.info(f\"Downloaded SentencePiece model to {model_path}\")\n        else:\n            raise Exception(f\"Failed to download model from {model_url}\")\n        return model_path\n    def encode(\n        self, s: str, modality: str, bos: bool = True, eos: bool = True\n    ) -> List[int]:\n        \"\"\"Encodes a string into a list of token IDs.\n        Args:\n            s (str): _description_\n            modality (str): _description_\n            bos (bool, optional): _description_. Defaults to True.\n            eos (bool, optional): _description_. Defaults to True.\n        Returns:"
        },
        {
            "comment": "This code defines a tokenizer that encodes and decodes strings into a list of token IDs. It supports modality tokens, BOS (Beginning Of Sequence), and EOS (End Of Sequence) tokens. The encode function prepends and appends modality tokens if available, and adds BOS/EOS tokens if required. The decode function removes modality tokens before decoding the list of token IDs into a string.",
            "location": "\"/media/root/Toshiba XG3/works/Gemini/docs/src/gemini_torch/tokenizer.py\":117-149",
            "content": "            List[int]: _description_\n        \"\"\"\n        assert isinstance(s, str)\n        tokens = self.sp_model.encode(s)\n        # Prepend start and append end modality tokens if available\n        modality_start_id, modality_end_id = self.modality_tokens.get(\n            modality, (-1, -1)\n        )\n        if modality_start_id != -1 and modality_end_id != -1:\n            tokens = [modality_start_id] + tokens + [modality_end_id]\n        # Add BOS/EOS tokens if required\n        if bos:\n            tokens = [self.bos_id] + tokens\n        if eos:\n            tokens = tokens + [self.eos_id]\n        return tokens\n    def decode(self, tokens: List[int]) -> str:\n        \"\"\"decodes a list of token IDs into a string.\n        Args:\n            tokens (List[int]): _description_\n        Returns:\n            str: _description_\n        \"\"\"\n        # Remove modality tokens before decoding\n        for start_id, end_id in self.modality_tokens.values():\n            tokens = [t for t in tokens if t not in (start_id, end_id)]\n        return self.sp_model.decode(tokens)"
        }
    ]
}