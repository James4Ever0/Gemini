{
    "summary": "The code includes a Transformer model with customizable layers, dropout, rotary position embeddings for autoregressive and non-autoregressive training, Memory Transformers support, and allows users to select attention mechanism outputs.",
    "details": [
        {
            "comment": "The code imports necessary libraries and defines a class `LayerIntermediates` with optional fields for various tensor lists and a loss tensor. It also includes several utility functions such as `exists`, `default`, `cast_tuple`, `divisible_by`, and `maybe`. These helpers will be used throughout the rest of the codebase to handle optional inputs and perform computations.",
            "location": "\"/media/root/Toshiba XG3/works/Gemini/docs/src/gemini_torch/transformer.py\":0-50",
            "content": "import math\nfrom dataclasses import dataclass\nfrom functools import partial, wraps\nfrom inspect import isfunction\nfrom random import random\nfrom typing import Callable, List, Optional\nimport torch\nimport torch.nn.functional as F\nfrom einops import rearrange, reduce, repeat\nfrom torch import Tensor, einsum, nn\nfrom zeta.nn.attention.attend import Attend, Intermediates\nDEFAULT_DIM_HEAD = 64\n@dataclass\nclass LayerIntermediates:\n    hiddens: Optional[List[Tensor]] = None\n    attn_intermediates: Optional[List[Intermediates]] = None\n    layer_hiddens: Optional[List[Tensor]] = None\n    attn_z_loss: Optional[Tensor] = None\n# helpers\ndef exists(val):\n    return val is not None\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if isfunction(d) else d\ndef cast_tuple(val, depth):\n    return val if isinstance(val, tuple) else (val,) * depth\ndef divisible_by(num, den):\n    return (num % den) == 0\ndef maybe(fn):\n    @wraps(fn)\n    def inner(x, *args, **kwargs):\n        if not exists(x):\n            return x\n        return fn(x, *args, **kwargs)"
        },
        {
            "comment": "The code contains classes for conditional operations, a Sequential function to create neural network architectures, and tensor manipulation functions like max_neg_value for clipping values, l2norm for normalizing tensor elements, pad_at_dim for padding tensors along specific dimensions, and or_reduce for logical OR operation reduction.",
            "location": "\"/media/root/Toshiba XG3/works/Gemini/docs/src/gemini_torch/transformer.py\":52-105",
            "content": "    return inner\nclass always:\n    def __init__(self, val):\n        self.val = val\n    def __call__(self, *args, **kwargs):\n        return self.val\nclass not_equals:\n    def __init__(self, val):\n        self.val = val\n    def __call__(self, x, *args, **kwargs):\n        return x != self.val\nclass equals:\n    def __init__(self, val):\n        self.val = val\n    def __call__(self, x, *args, **kwargs):\n        return x == self.val\ndef Sequential(*modules):\n    return nn.Sequential(*filter(exists, modules))\n# tensor helpers\ndef max_neg_value(tensor):\n    return -torch.finfo(tensor.dtype).max\ndef l2norm(t, groups=1):\n    t = rearrange(t, \"... (g d) -> ... g d\", g=groups)\n    t = F.normalize(t, p=2, dim=-1)\n    return rearrange(t, \"... g d -> ... (g d)\")\ndef pad_at_dim(t, pad, dim=-1, value=0.0):\n    dims_from_right = (-dim - 1) if dim < 0 else (t.ndim - dim - 1)\n    zeros = (0, 0) * dims_from_right\n    return F.pad(t, (*zeros, *pad), value=value)\ndef or_reduce(masks):\n    head, *body = masks\n    for rest in body:\n        head = head | rest"
        },
        {
            "comment": "The code snippet contains several functions related to transformers. It defines a function `calc_z_loss` that calculates the loss based on attention logits, used in papers like https://arxiv.org/abs/2202.08906 and employed by PaLM. There is also an `init_zero_` function initializing layers with zeros, and two helper functions: `pick_and_pop` for popping values from a dictionary using keys, and `group_dict_by_key` for grouping dictionary items based on specific keys.",
            "location": "\"/media/root/Toshiba XG3/works/Gemini/docs/src/gemini_torch/transformer.py\":106-150",
            "content": "    return head\n# auxiliary loss helpers\ndef calc_z_loss(pre_softmax_attns: List[Tensor], mask=None, weight=1.0):\n    # the same loss applied to the mixture of experts router logits in https://arxiv.org/abs/2202.08906\n    # in the paper, in a tiny footnote, they mention using it on attention logits with stabilizing effects\n    # also used in PaLM as one of the measures\n    lse = 0.0\n    for attn in pre_softmax_attns:\n        lse = lse + attn.logsumexp(dim=-1)\n    loss = torch.square(lse)\n    loss = reduce(loss, \"b h n -> b n\", \"sum\")\n    if not exists(mask):\n        return loss.mean() * weight\n    loss = loss[mask].sum() / mask.sum().clamp(min=1e-5)\n    return loss * weight\n# init helpers\ndef init_zero_(layer):\n    nn.init.constant_(layer.weight, 0.0)\n    if exists(layer.bias):\n        nn.init.constant_(layer.bias, 0.0)\n# keyword argument helpers\ndef pick_and_pop(keys, d):\n    values = list(map(lambda key: d.pop(key), keys))\n    return dict(zip(keys, values))\ndef group_dict_by_key(cond, d):\n    return_val = [dict(), dict()]"
        },
        {
            "comment": "This code defines functions for grouping dictionary keys by prefix, initializing a transformer with deep normalization, and setting module weights based on certain conditions. The \"deepnorm_init\" function iterates through the named modules of a transformer, checks if they are of type nn.Linear, determines if they need to be initialized with beta gain based on their names, and then sets their weight data using xavier_normal_ initializer with the appropriate gain value.",
            "location": "\"/media/root/Toshiba XG3/works/Gemini/docs/src/gemini_torch/transformer.py\":151-190",
            "content": "    for key in d.keys():\n        match = bool(cond(key))\n        ind = int(not match)\n        return_val[ind][key] = d[key]\n    return (*return_val,)\ndef string_begins_with(prefix, str):\n    return str.startswith(prefix)\ndef group_by_key_prefix(prefix, d):\n    return group_dict_by_key(partial(string_begins_with, prefix), d)\ndef groupby_prefix_and_trim(prefix, d):\n    kwargs_with_prefix, kwargs = group_dict_by_key(\n        partial(string_begins_with, prefix), d\n    )\n    kwargs_without_prefix = dict(\n        map(lambda x: (x[0][len(prefix) :], x[1]), tuple(kwargs_with_prefix.items()))\n    )\n    return kwargs_without_prefix, kwargs\n# initializations\ndef deepnorm_init(\n    transformer, beta, module_name_match_list=[\".ff.\", \".to_v\", \".to_out\"]\n):\n    for name, module in transformer.named_modules():\n        if type(module) != nn.Linear:\n            continue\n        needs_beta_gain = any(\n            map(lambda substr: substr in name, module_name_match_list)\n        )\n        gain = beta if needs_beta_gain else 1\n        nn.init.xavier_normal_(module.weight.data, gain=gain)"
        },
        {
            "comment": "The code snippet initializes the module's bias with a constant value of 0, and defines a dropout_seq function that applies structured dropout to sequences while considering mask. It generates random logits, selects a specified number of keep indices, assigns them based on batch indices, adjusts the mask if present, and finally returns the sequence and mask after applying dropout.",
            "location": "\"/media/root/Toshiba XG3/works/Gemini/docs/src/gemini_torch/transformer.py\":192-228",
            "content": "        if exists(module.bias):\n            nn.init.constant_(module.bias.data, 0)\n# structured dropout, more effective than traditional attention dropouts\ndef dropout_seq(seq, mask, dropout):\n    b, n, *_, device = *seq.shape, seq.device\n    logits = torch.randn(b, n, device=device)\n    if exists(mask):\n        mask_value = max_neg_value(logits)\n        logits = logits.masked_fill(~mask, mask_value)\n    keep_prob = 1.0 - dropout\n    num_keep = max(1, int(keep_prob * n))\n    keep_indices = logits.topk(num_keep, dim=1).indices\n    batch_indices = torch.arange(b, device=device)\n    batch_indices = rearrange(batch_indices, \"b -> b 1\")\n    seq = seq[batch_indices, keep_indices]\n    if exists(mask):\n        seq_counts = mask.sum(dim=-1)\n        seq_keep_counts = torch.ceil(seq_counts * keep_prob).int()\n        keep_mask = torch.arange(num_keep, device=device) < rearrange(\n            seq_keep_counts, \"b -> b 1\"\n        )\n        mask = mask[batch_indices, keep_indices] & keep_mask\n    return seq, mask\n# activations"
        },
        {
            "comment": "ReluSquared: applies ReLU activation function and squares the output.\nTokenEmbedding: embedding layer for tokens, optionally applies L2-normalization.\nAbsolutePositionalEmbedding: adds absolute positional embeddings to input, optionally L2-normalizes.",
            "location": "\"/media/root/Toshiba XG3/works/Gemini/docs/src/gemini_torch/transformer.py\":231-265",
            "content": "class ReluSquared(nn.Module):\n    def forward(self, x):\n        return F.relu(x) ** 2\n# embedding\nclass TokenEmbedding(nn.Module):\n    def __init__(self, dim, num_tokens, l2norm_embed=False):\n        super().__init__()\n        self.l2norm_embed = l2norm_embed\n        self.emb = nn.Embedding(num_tokens, dim)\n    def forward(self, x):\n        token_emb = self.emb(x)\n        return l2norm(token_emb) if self.l2norm_embed else token_emb\n# positional embeddings\nclass AbsolutePositionalEmbedding(nn.Module):\n    def __init__(self, dim, max_seq_len, l2norm_embed=False):\n        super().__init__()\n        self.scale = dim**-0.5 if not l2norm_embed else 1.0\n        self.max_seq_len = max_seq_len\n        self.l2norm_embed = l2norm_embed\n        self.emb = nn.Embedding(max_seq_len, dim)\n    def forward(self, x, pos=None):\n        seq_len, device = x.shape[1], x.device\n        assert (\n            seq_len <= self.max_seq_len\n        ), f\"you are passing in a sequence length of {seq_len} but your absolute positional embedding has a max sequence length of {self.max_seq_len}\""
        },
        {
            "comment": "The code defines a class for scaled sinusoidal positional embeddings and a relative position bias. The embeddings are computed using sinusoidal functions of the form x = a * sin(2\u03c0b * k / q) + c, where 'a', 'b', 'c' and 'q' are constants, and 'k' is the frequency index. These embeddings are then normalized by dividing by the square root of their dimension. The relative position bias class computes positional embeddings for each position, taking into account the distance between positions in a sequence. It uses buckets to group similar distances together and applies a different scaling factor to each bucket.",
            "location": "\"/media/root/Toshiba XG3/works/Gemini/docs/src/gemini_torch/transformer.py\":267-298",
            "content": "        if not exists(pos):\n            pos = torch.arange(seq_len, device=device)\n        pos_emb = self.emb(pos)\n        pos_emb = pos_emb * self.scale\n        return l2norm(pos_emb) if self.l2norm_embed else pos_emb\nclass ScaledSinusoidalEmbedding(nn.Module):\n    def __init__(self, dim, theta=10000):\n        super().__init__()\n        assert divisible_by(dim, 2)\n        self.scale = nn.Parameter(torch.ones(1) * dim**-0.5)\n        half_dim = dim // 2\n        freq_seq = torch.arange(half_dim).float() / half_dim\n        inv_freq = theta**-freq_seq\n        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n    def forward(self, x, pos=None):\n        seq_len, device = x.shape[1], x.device\n        if not exists(pos):\n            pos = torch.arange(seq_len, device=device)\n        emb = einsum(\"i, j -> i j\", pos, self.inv_freq)\n        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n        return emb * self.scale\nclass RelativePositionBias(nn.Module):\n    def __init__(self, scale, causal=False, num_buckets=32, max_distance=128, heads=8):"
        },
        {
            "comment": "This code initializes a class with parameters like scale, causal, num_buckets, and max_distance. It also defines an embedding layer for relative attention bias. The _relative_position_bucket method calculates the position bucket based on the given position, considering causality and position constraints.",
            "location": "\"/media/root/Toshiba XG3/works/Gemini/docs/src/gemini_torch/transformer.py\":299-332",
            "content": "        super().__init__()\n        self.scale = scale\n        self.causal = causal\n        self.num_buckets = num_buckets\n        self.max_distance = max_distance\n        self.relative_attention_bias = nn.Embedding(num_buckets, heads)\n    @staticmethod\n    def _relative_position_bucket(\n        relative_position, causal=True, num_buckets=32, max_distance=128\n    ):\n        ret = 0\n        n = -relative_position\n        if not causal:\n            num_buckets //= 2\n            ret += (n < 0).long() * num_buckets\n            n = torch.abs(n)\n        else:\n            n = torch.max(n, torch.zeros_like(n))\n        max_exact = num_buckets // 2\n        is_small = n < max_exact\n        val_if_large = (\n            max_exact\n            + (\n                torch.log(n.float() / max_exact)\n                / math.log(max_distance / max_exact)\n                * (num_buckets - max_exact)\n            ).long()\n        )\n        val_if_large = torch.min(\n            val_if_large, torch.full_like(val_if_large, num_buckets - 1)\n        )"
        },
        {
            "comment": "This code defines a DynamicPositionBias class, which is used to calculate position biases for attention mechanisms in transformers. The forward function takes input indices i and j, and returns a bias tensor that will be multiplied by the attention scores. This helps improve the model's understanding of relative positions between tokens.",
            "location": "\"/media/root/Toshiba XG3/works/Gemini/docs/src/gemini_torch/transformer.py\":334-363",
            "content": "        ret += torch.where(is_small, n, val_if_large)\n        return ret\n    @property\n    def device(self):\n        return next(self.parameters()).device\n    def forward(self, i, j):\n        device = self.device\n        q_pos = torch.arange(j - i, j, dtype=torch.long, device=device)\n        k_pos = torch.arange(j, dtype=torch.long, device=device)\n        rel_pos = k_pos[None, :] - q_pos[:, None]\n        rp_bucket = self._relative_position_bucket(\n            rel_pos,\n            causal=self.causal,\n            num_buckets=self.num_buckets,\n            max_distance=self.max_distance,\n        )\n        values = self.relative_attention_bias(rp_bucket)\n        bias = rearrange(values, \"i j h -> h i j\")\n        return bias * self.scale\nclass DynamicPositionBias(nn.Module):\n    def __init__(self, dim, *, heads, depth, log_distance=False, norm=False):\n        super().__init__()\n        assert (\n            depth >= 1\n        ), \"depth for dynamic position bias MLP must be greater or equal to 1\"\n        self.log_distance = log_distance"
        },
        {
            "comment": "The code defines a class with an MLP (Multi-Layer Perceptron) used for position embeddings in a transformer model. The MLP takes an input dimension and applies a sequence of linear layers, layer normalization (optional), and SiLU activation function. It also has properties to get the device and a forward method that calculates the position embeddings based on sequence and context ranges.",
            "location": "\"/media/root/Toshiba XG3/works/Gemini/docs/src/gemini_torch/transformer.py\":365-400",
            "content": "        self.mlp = nn.ModuleList([])\n        self.mlp.append(\n            Sequential(\n                nn.Linear(1, dim), nn.LayerNorm(dim) if norm else None, nn.SiLU()\n            )\n        )\n        for _ in range(depth - 1):\n            self.mlp.append(\n                Sequential(\n                    nn.Linear(dim, dim), nn.LayerNorm(dim) if norm else None, nn.SiLU()\n                )\n            )\n        self.mlp.append(nn.Linear(dim, heads))\n    @property\n    def device(self):\n        return next(self.parameters()).device\n    def forward(self, i, j):\n        assert i == j\n        n, device = j, self.device\n        # get the (n x n) matrix of distances\n        seq_arange = torch.arange(n, device=device)\n        context_arange = torch.arange(n, device=device)\n        indices = rearrange(seq_arange, \"i -> i 1\") - rearrange(\n            context_arange, \"j -> 1 j\"\n        )\n        indices += n - 1\n        # input to continuous positions MLP\n        pos = torch.arange(-n + 1, n, device=device).float()\n        pos = rearrange(pos, \"... -> ... 1\")"
        },
        {
            "comment": "The code defines a class called AlibiPositionalBias that inherits from nn.Module. This class calculates and returns positional biases for some transformer model layer. It uses torch functions like sign and log to manipulate the positions, then applies some rearrangement operations to reshape tensors. The code also utilizes a register_buffer to store slopes and bias variables.",
            "location": "\"/media/root/Toshiba XG3/works/Gemini/docs/src/gemini_torch/transformer.py\":402-431",
            "content": "        if self.log_distance:\n            pos = torch.sign(pos) * torch.log(\n                pos.abs() + 1\n            )  # log of distance is sign(rel_pos) * log(abs(rel_pos) + 1)\n        for layer in self.mlp:\n            pos = layer(pos)\n        # get position biases\n        bias = pos[indices]\n        bias = rearrange(bias, \"i j h -> h i j\")\n        return bias\nclass AlibiPositionalBias(nn.Module):\n    def __init__(self, heads, total_heads, **kwargs):\n        super().__init__()\n        self.heads = heads\n        self.total_heads = total_heads\n        slopes = Tensor(self._get_slopes(heads))\n        slopes = rearrange(slopes, \"h -> h 1 1\")\n        self.register_buffer(\"slopes\", slopes, persistent=False)\n        self.register_buffer(\"bias\", None, persistent=False)\n    def get_bias(self, i, j, device):\n        i_arange = torch.arange(j - i, j, device=device)\n        j_arange = torch.arange(j, device=device)\n        bias = -torch.abs(\n            rearrange(j_arange, \"j -> 1 1 j\") - rearrange(i_arange, \"i -> 1 i 1\")"
        },
        {
            "comment": "This code defines a class with methods for getting and setting biases based on head counts. It uses logarithmic calculations to determine slopes for the biases, and ensures correct device assignment. The `forward` method handles retrieving or generating biases and padding if necessary.",
            "location": "\"/media/root/Toshiba XG3/works/Gemini/docs/src/gemini_torch/transformer.py\":432-467",
            "content": "        )\n        return bias\n    @staticmethod\n    def _get_slopes(heads):\n        def get_slopes_power_of_2(n):\n            start = 2 ** (-(2 ** -(math.log2(n) - 3)))\n            ratio = start\n            return [start * ratio**i for i in range(n)]\n        if math.log2(heads).is_integer():\n            return get_slopes_power_of_2(heads)\n        closest_power_of_2 = 2 ** math.floor(math.log2(heads))\n        return (\n            get_slopes_power_of_2(closest_power_of_2)\n            + get_slopes_power_of_2(2 * closest_power_of_2)[0::2][\n                : heads - closest_power_of_2\n            ]\n        )\n    @property\n    def device(self):\n        return next(self.buffers()).device\n    def forward(self, i, j):\n        h, device = self.total_heads, self.device\n        if exists(self.bias) and self.bias.shape[-1] >= j and self.bias.shape[-2] >= i:\n            return self.bias[..., :i, :j]\n        bias = self.get_bias(i, j, device)\n        bias = bias * self.slopes\n        num_heads_unalibied = h - bias.shape[0]\n        bias = pad_at_dim(bias, (0, num_heads_unalibied), dim=0)"
        },
        {
            "comment": "This code registers a buffer \"bias\" to the class and returns it. It then initializes a RotaryEmbedding class with parameters like dim, use_xpos, scale_base, interpolation_factor, and base_rescale_factor. If use_xpos is True, it also creates a \"scale\" buffer.",
            "location": "\"/media/root/Toshiba XG3/works/Gemini/docs/src/gemini_torch/transformer.py\":468-499",
            "content": "        self.register_buffer(\"bias\", bias, persistent=False)\n        return self.bias\nclass RotaryEmbedding(nn.Module):\n    def __init__(\n        self,\n        dim,\n        use_xpos=False,\n        scale_base=512,\n        interpolation_factor=1.0,\n        base=10000,\n        base_rescale_factor=1.0,\n    ):\n        super().__init__()\n        # proposed by reddit user bloc97, to rescale rotary embeddings to longer sequence length without fine-tuning\n        # has some connection to NTK literature\n        # https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/\n        base *= base_rescale_factor ** (dim / (dim - 2))\n        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n        self.register_buffer(\"inv_freq\", inv_freq)\n        assert interpolation_factor >= 1.0\n        self.interpolation_factor = interpolation_factor\n        if not use_xpos:\n            self.register_buffer(\"scale\", None)\n            return\n        scale = (torch.arange(0, dim, 2) + 0.4 * dim) / (1.4 * dim)"
        },
        {
            "comment": "This code defines a class called `Scale` that initializes a scale value and a function (`fn`) in its constructor. The `forward` method computes the rotary position embedding by applying a rotation to the input sequence based on the provided frequency, interpolation factor, and scale value. The `rotate_half` function splits the input tensor along dimension -2 and concatenates the second half with a negative sign. Finally, the `apply_rotary_pos_emb` function computes the rotary position embedding using trigonometric functions (cosine and sine).",
            "location": "\"/media/root/Toshiba XG3/works/Gemini/docs/src/gemini_torch/transformer.py\":501-539",
            "content": "        self.scale_base = scale_base\n        self.register_buffer(\"scale\", scale)\n    def forward(self, seq_len, device):\n        t = torch.arange(seq_len, device=device).type_as(self.inv_freq)\n        t = t / self.interpolation_factor\n        freqs = torch.einsum(\"i , j -> i j\", t, self.inv_freq)\n        freqs = torch.cat((freqs, freqs), dim=-1)\n        if not exists(self.scale):\n            return freqs, 1.0\n        power = (\n            torch.arange(seq_len, device=device) - (seq_len // 2)\n        ) / self.scale_base\n        scale = self.scale ** rearrange(power, \"n -> n 1\")\n        scale = torch.cat((scale, scale), dim=-1)\n        return freqs, scale\ndef rotate_half(x):\n    x = rearrange(x, \"... (j d) -> ... j d\", j=2)\n    x1, x2 = x.unbind(dim=-2)\n    return torch.cat((-x2, x1), dim=-1)\ndef apply_rotary_pos_emb(t, freqs, scale=1):\n    seq_len = t.shape[-2]\n    freqs = freqs[-seq_len:, :]\n    return (t * freqs.cos() * scale) + (rotate_half(t) * freqs.sin() * scale)\n# norms\nclass Scale(nn.Module):\n    def __init__(self, value, fn):"
        },
        {
            "comment": "This code defines several custom normalization layers (ScaleNorm, RMSNorm, and SimpleRMSNorm) that can be used with PyTorch. These layers perform different types of normalization operations on input tensors. The forward method in each class defines how the normalization is performed. ScaleNorm scales the input by dividing it by its norm and then scaling by a learnable parameter g. RMSNorm first performs instance-wise normalization using F.normalize, then scales by a learnable parameter g and a square root of the dimension size. SimpleRMSNorm performs instance-wise normalization without any scale parameters.",
            "location": "\"/media/root/Toshiba XG3/works/Gemini/docs/src/gemini_torch/transformer.py\":540-583",
            "content": "        super().__init__()\n        self.value = value\n        self.fn = fn\n    def forward(self, x, **kwargs):\n        out = self.fn(x, **kwargs)\n        def scale_fn(t):\n            return t * self.value\n        if not isinstance(out, tuple):\n            return scale_fn(out)\n        return (scale_fn(out[0]), *out[1:])\nclass ScaleNorm(nn.Module):\n    def __init__(self, dim, eps=1e-5):\n        super().__init__()\n        self.eps = eps\n        self.g = nn.Parameter(torch.ones(1) * (dim**-0.5))\n    def forward(self, x):\n        norm = torch.norm(x, dim=-1, keepdim=True)\n        return x / norm.clamp(min=self.eps) * self.g\nclass RMSNorm(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.scale = dim**0.5\n        self.g = nn.Parameter(torch.ones(dim))\n    def forward(self, x):\n        return F.normalize(x, dim=-1) * self.scale * self.g\nclass SimpleRMSNorm(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.scale = dim**0.5\n    def forward(self, x):\n        return F.normalize(x, dim=-1) * self.scale"
        },
        {
            "comment": "This code defines two classes: Residual and GRUGating. Residual class handles residual connections with optional scaling, while GRUGating class implements a Gated Recurrent Unit (GRU) gating mechanism for processing sequences. The forward functions scale the residuals if needed before adding them to the input, and the GRU cell processes the input and residual in batched format.",
            "location": "\"/media/root/Toshiba XG3/works/Gemini/docs/src/gemini_torch/transformer.py\":586-616",
            "content": "# residual and residual gates\nclass Residual(nn.Module):\n    def __init__(self, dim, scale_residual=False, scale_residual_constant=1.0):\n        super().__init__()\n        self.residual_scale = nn.Parameter(torch.ones(dim)) if scale_residual else None\n        self.scale_residual_constant = scale_residual_constant\n    def forward(self, x, residual):\n        if exists(self.residual_scale):\n            residual = residual * self.residual_scale\n        if self.scale_residual_constant != 1:\n            residual = residual * self.scale_residual_constant\n        return x + residual\nclass GRUGating(nn.Module):\n    def __init__(self, dim, scale_residual=False, **kwargs):\n        super().__init__()\n        self.gru = nn.GRUCell(dim, dim)\n        self.residual_scale = nn.Parameter(torch.ones(dim)) if scale_residual else None\n    def forward(self, x, residual):\n        if exists(self.residual_scale):\n            residual = residual * self.residual_scale\n        gated_output = self.gru(\n            rearrange(x, \"b n d -> (b n) d\"), rearrange(residual, \"b n d -> (b n) d\")"
        },
        {
            "comment": "This code defines a ShiftTokens module which shifts certain segments of input features by a specified amount. It also includes a GLU (Gated Linear Units) function, and the code snippet demonstrates how to use these components in a FeedForward class. The feedforward operation applies both gating and linear operations on its input, effectively combining information from different depths of the encoder-decoder layers.",
            "location": "\"/media/root/Toshiba XG3/works/Gemini/docs/src/gemini_torch/transformer.py\":617-660",
            "content": "        )\n        return gated_output.reshape_as(x)\n# token shifting\ndef shift(t, amount, mask=None):\n    if amount == 0:\n        return t\n    else:\n        amount = min(amount, t.shape[1])\n    if exists(mask):\n        t = t.masked_fill(~mask[..., None], 0.0)\n    return pad_at_dim(t, (amount, -amount), dim=-2, value=0.0)\nclass ShiftTokens(nn.Module):\n    def __init__(self, shifts, fn):\n        super().__init__()\n        self.fn = fn\n        self.shifts = tuple(shifts)\n    def forward(self, x, **kwargs):\n        mask = kwargs.get(\"mask\", None)\n        shifts = self.shifts\n        segments = len(shifts)\n        feats_per_shift = x.shape[-1] // segments\n        splitted = x.split(feats_per_shift, dim=-1)\n        segments_to_shift, rest = splitted[:segments], splitted[segments:]\n        segments_to_shift = list(\n            map(lambda args: shift(*args, mask=mask), zip(segments_to_shift, shifts))\n        )\n        x = torch.cat((*segments_to_shift, *rest), dim=-1)\n        return self.fn(x, **kwargs)\n# feedforward\nclass GLU(nn.Module):"
        },
        {
            "comment": "This code initializes a FeedForward module with an optional dimension output, multiplier, and activation function. It contains classes for the FeedForward module and its initialization process. The activation function can be GELU, SiLU or ReLuSquared depending on the input parameters. It also includes options for post-activation layer normalization and dropout.",
            "location": "\"/media/root/Toshiba XG3/works/Gemini/docs/src/gemini_torch/transformer.py\":661-699",
            "content": "    def __init__(self, dim_in, dim_out, activation: Callable, mult_bias=False):\n        super().__init__()\n        self.act = activation\n        self.proj = nn.Linear(dim_in, dim_out * 2)\n        self.mult_bias = nn.Parameter(torch.ones(dim_out)) if mult_bias else 1.0\n    def forward(self, x):\n        x, gate = self.proj(x).chunk(2, dim=-1)\n        return x * self.act(gate) * self.mult_bias\nclass FeedForward(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_out=None,\n        mult=4,\n        glu=False,\n        glu_mult_bias=False,\n        swish=False,\n        relu_squared=False,\n        post_act_ln=False,\n        dropout=0.0,\n        no_bias=False,\n        zero_init_output=False,\n    ):\n        super().__init__()\n        inner_dim = int(dim * mult)\n        dim_out = default(dim_out, dim)\n        if relu_squared:\n            activation = ReluSquared()\n        elif swish:\n            activation = nn.SiLU()\n        else:\n            activation = nn.GELU()\n        if glu:\n            project_in = GLU(dim, inner_dim, activation, mult_bias=glu_mult_bias)"
        },
        {
            "comment": "The code defines a module for self-attention and feed-forward layers in a transformer architecture. It initializes the layers with specified dimensions, includes optional parameters for dropout, layer normalization, and other features like causal and flash attention. The forward function performs the attention calculation and returns the output.",
            "location": "\"/media/root/Toshiba XG3/works/Gemini/docs/src/gemini_torch/transformer.py\":700-743",
            "content": "        else:\n            project_in = nn.Sequential(\n                nn.Linear(dim, inner_dim, bias=not no_bias), activation\n            )\n        self.ff = Sequential(\n            project_in,\n            nn.LayerNorm(inner_dim) if post_act_ln else None,\n            nn.Dropout(dropout),\n            nn.Linear(inner_dim, dim_out, bias=not no_bias),\n        )\n        # init last linear layer to 0\n        if zero_init_output:\n            init_zero_(self.ff[-1])\n    def forward(self, x):\n        return self.ff(x)\n# attention. it is all we need\nclass Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_head=DEFAULT_DIM_HEAD,\n        heads=8,\n        causal=False,\n        flash=False,\n        talking_heads=False,\n        head_scale=False,\n        sparse_topk=None,\n        num_mem_kv=0,\n        dropout=0.0,\n        on_attn=False,\n        gate_values=False,\n        zero_init_output=False,\n        max_attend_past=None,\n        qk_norm=False,\n        qk_norm_groups=1,\n        qk_norm_scale=10,\n        qk_norm_dim_scale=False,"
        },
        {
            "comment": "This code initializes a transformer layer with configurable parameters such as the number of heads, causal masking, maximum attended past steps, and more. The code checks for inconsistencies in the input arguments and calculates dimensions for query (Q), key (K), and value (V) matrices based on the provided inputs.",
            "location": "\"/media/root/Toshiba XG3/works/Gemini/docs/src/gemini_torch/transformer.py\":744-775",
            "content": "        one_kv_head=False,\n        kv_heads=None,\n        shared_kv=False,\n        value_dim_head=None,\n        tensor_product=False,  # https://arxiv.org/abs/2208.06061\n        cascading_heads=False,\n        add_zero_kv=False,  # same as add_zero_attn in pytorch\n        onnxable=False,\n    ):\n        super().__init__()\n        self.scale = dim_head**-0.5\n        self.heads = heads\n        self.causal = causal\n        self.max_attend_past = max_attend_past\n        assert not (\n            exists(kv_heads) and one_kv_head\n        ), \"either attn_one_kv_head is set to True (in which case kv_heads is set to 1), or attn_kv_heads is set, but not both\"\n        value_dim_head = default(value_dim_head, dim_head)\n        kv_heads = default(kv_heads, heads)\n        kv_heads = 1 if one_kv_head else kv_heads\n        assert divisible_by(heads, kv_heads)\n        self.kv_heads = kv_heads\n        q_dim = dim_head * heads\n        k_dim = dim_head * kv_heads\n        v_dim = value_dim_head * kv_heads\n        out_dim = value_dim_head * heads"
        },
        {
            "comment": "The code initializes layers for a transformer model. It includes linear layers for query (to_q), key (to_k) and value (to_v) projections, as well as a relation projection layer (to_r). The code also checks for shared key/value dimensions and whether to use gating for aggregated values (to_v_gate). Additionally, it sets parameters for cosine similarity attention (qk_norm, qk_norm_groups, qk_norm_scale) and determines if RMSNorm should be used.",
            "location": "\"/media/root/Toshiba XG3/works/Gemini/docs/src/gemini_torch/transformer.py\":777-801",
            "content": "        self.to_q = nn.Linear(dim, q_dim, bias=False)\n        self.to_k = nn.Linear(dim, k_dim, bias=False)\n        # shared key / values, for further memory savings during inference\n        assert not (\n            shared_kv and value_dim_head != dim_head\n        ), \"key and value head dimensions must be equal for shared key / values\"\n        self.to_v = nn.Linear(dim, v_dim, bias=False) if not shared_kv else None\n        # relations projection from tp-attention\n        self.to_r = nn.Linear(dim, v_dim, bias=False) if tensor_product else None\n        # add GLU gating for aggregated values, from alphafold2\n        self.to_v_gate = None\n        if gate_values:\n            self.to_v_gate = nn.Linear(dim, out_dim)\n            nn.init.constant_(self.to_v_gate.weight, 0)\n            nn.init.constant_(self.to_v_gate.bias, 1)\n        # cosine sim attention\n        self.qk_norm = qk_norm\n        self.qk_norm_groups = qk_norm_groups\n        self.qk_norm_scale = qk_norm_scale\n        # whether to use the rmsnorm (equivalent to cosine sim attention when scale is equal to 1) - https://arxiv.org/abs/2302.05442"
        },
        {
            "comment": "This code initializes the Transformer's attention module with optional normalization for query and key dimensions, and applies the core attention algorithm. It also includes parameters for causal masking, talking heads, dropout, sparse top-k sampling, and whether to add zero vectors for keys and values.",
            "location": "\"/media/root/Toshiba XG3/works/Gemini/docs/src/gemini_torch/transformer.py\":802-826",
            "content": "        self.qk_norm_dim_scale = qk_norm_dim_scale\n        self.qk_norm_q_scale = self.qk_norm_k_scale = 1\n        if qk_norm and qk_norm_dim_scale:\n            self.qk_norm_q_scale = nn.Parameter(torch.ones(dim_head))\n            self.qk_norm_k_scale = nn.Parameter(torch.ones(dim_head))\n        assert (not qk_norm) or divisible_by(\n            dim_head, qk_norm_groups\n        ), \"dimension per attention head must be divisible by the qk norm groups\"\n        assert not (\n            qk_norm and (dim_head // qk_norm_groups) <= 2\n        ), \"the group dimension may be too small (2 was too small in my tests, but 4 still works, surprisingly)\"\n        # attend class - includes core attention algorithm + talking heads\n        self.attend = Attend(\n            heads=heads,\n            causal=causal,\n            talking_heads=talking_heads,\n            dropout=dropout,\n            sparse_topk=sparse_topk,\n            qk_norm=qk_norm,\n            scale=qk_norm_scale if qk_norm else self.scale,\n            add_zero_kv=add_zero_kv,"
        },
        {
            "comment": "This code creates a transformer model with optional features like head scaling, sparse attention, memory key/values, and attention on attention. It initializes various layers and parameters based on provided arguments. The forward function performs the forward pass through these layers to output the final result.",
            "location": "\"/media/root/Toshiba XG3/works/Gemini/docs/src/gemini_torch/transformer.py\":827-862",
            "content": "            flash=flash,\n            onnxable=onnxable,\n        )\n        # head scaling\n        self.head_scale = head_scale\n        if head_scale:\n            self.head_scale_params = nn.Parameter(torch.ones(1, heads, 1, 1))\n        # explicit topk sparse attention\n        self.sparse_topk = sparse_topk\n        # add memory key / values\n        self.num_mem_kv = num_mem_kv\n        if num_mem_kv > 0:\n            self.mem_k = nn.Parameter(torch.randn(heads, num_mem_kv, dim_head))\n            self.mem_v = nn.Parameter(torch.randn(heads, num_mem_kv, dim_head))\n        # attention on attention\n        self.attn_on_attn = on_attn\n        self.to_out = (\n            nn.Sequential(nn.Linear(out_dim, dim * 2, bias=False), nn.GLU())\n            if on_attn\n            else nn.Linear(out_dim, dim, bias=False)\n        )\n        # init output projection 0\n        if zero_init_output:\n            init_zero_(self.to_out)\n    def forward(\n        self,\n        x,\n        context=None,\n        mask=None,\n        context_mask=None,"
        },
        {
            "comment": "This function takes in a multi-dimensional tensor `x` and optional context `context`. It assigns various shapes, variables, and device information from the input. The function also handles cases where memory is passed. It then applies transformations to the input tensor `x`, splitting it into query (q), key (k), value (v), and relative position (r) inputs. The function rearranges the shape of the inputs and applies optional normalization before returning them.",
            "location": "\"/media/root/Toshiba XG3/works/Gemini/docs/src/gemini_torch/transformer.py\":863-900",
            "content": "        attn_mask=None,\n        rel_pos=None,\n        rotary_pos_emb=None,\n        prev_attn=None,\n        mem=None,\n    ):\n        b, n, _, h, kv_h, head_scale, device, has_context = (\n            *x.shape,\n            self.heads,\n            self.kv_heads,\n            self.head_scale,\n            x.device,\n            exists(context),\n        )\n        kv_input = default(context, x)\n        q_input = x\n        k_input = kv_input\n        v_input = kv_input\n        r_input = x\n        if exists(mem):\n            k_input = torch.cat((mem, k_input), dim=-2)\n            v_input = torch.cat((mem, v_input), dim=-2)\n        q = self.to_q(q_input)\n        k = self.to_k(k_input)\n        v = self.to_v(v_input) if exists(self.to_v) else k\n        r = self.to_r(r_input) if exists(self.to_r) else None\n        q = rearrange(q, \"b n (h d) -> b h n d\", h=h)\n        k, v, r = map(\n            lambda t: maybe(rearrange)(t, \"b n (h d) -> b h n d\", h=kv_h), (k, v, r)\n        )\n        if self.qk_norm:\n            qk_l2norm = partial(l2norm, groups=self.qk_norm_groups)"
        },
        {
            "comment": "This code performs linear attention over a set of key-value pairs (q,k,v) and optionally applies rotary position embeddings to the queries (q) and keys (k). It also handles a possible input mask and applies memory keys if mem_k and mem_v are present. The qk_norm_q_scale and qk_norm_k_scale variables are used for normalization of the queries and keys, respectively.",
            "location": "\"/media/root/Toshiba XG3/works/Gemini/docs/src/gemini_torch/transformer.py\":901-929",
            "content": "            q, k = map(qk_l2norm, (q, k))\n            q = q * self.qk_norm_q_scale\n            k = k * self.qk_norm_k_scale\n        if exists(rotary_pos_emb) and not has_context:\n            freqs, xpos_scale = rotary_pos_emb\n            l = freqs.shape[-1]\n            q_xpos_scale, k_xpos_scale = (\n                (xpos_scale, xpos_scale**-1.0) if exists(xpos_scale) else (1.0, 1.0)\n            )\n            (ql, qr), (kl, kr), (vl, vr) = map(\n                lambda t: (t[..., :l], t[..., l:]), (q, k, v)\n            )\n            ql, kl, vl = map(\n                lambda arg: apply_rotary_pos_emb(arg[0], freqs, arg[1]),\n                ((ql, q_xpos_scale), (kl, k_xpos_scale), (vl, k_xpos_scale)),\n            )\n            q, k, v = map(\n                lambda t: torch.cat(t, dim=-1), ((ql, qr), (kl, kr), (vl, vr))\n            )\n        input_mask = context_mask if has_context else mask\n        if self.num_mem_kv > 0:\n            mem_k, mem_v = map(\n                lambda t: repeat(t, \"h n d -> b h n d\", b=b), (self.mem_k, self.mem_v)"
        },
        {
            "comment": "This code section normalizes the memory keys and values based on qk_norm, concatenates them with existing keys and values, masks the attention matrix using input_mask and attn_mask, and reshapes the masks. It then prepares for the attention calculation by determining the dimensions i and j of query (q).",
            "location": "\"/media/root/Toshiba XG3/works/Gemini/docs/src/gemini_torch/transformer.py\":930-963",
            "content": "            )\n            if self.qk_norm:\n                mem_k = l2norm(mem_k)\n                mem_k = mem_k * self.qk_norm_k_scale\n            k = torch.cat((mem_k, k), dim=-2)\n            v = torch.cat((mem_v, v), dim=-2)\n            if exists(input_mask):\n                input_mask = pad_at_dim(\n                    input_mask, (self.num_mem_kv, 0), dim=-1, value=True\n                )\n        i, j = map(lambda t: t.shape[-2], (q, k))\n        # determine masking\n        max_neg_value(q)\n        masks = []\n        final_attn_mask = None\n        if exists(input_mask):\n            input_mask = rearrange(input_mask, \"b j -> b 1 1 j\")\n            masks.append(~input_mask)\n        if exists(attn_mask):\n            assert (\n                2 <= attn_mask.ndim <= 4\n            ), \"attention mask must have greater than 2 dimensions but less than or equal to 4\"\n            if attn_mask.ndim == 2:\n                attn_mask = rearrange(attn_mask, \"i j -> 1 1 i j\")\n            elif attn_mask.ndim == 3:\n                attn_mask = rearrange(attn_mask, \"h i j -> 1 h i j\")"
        },
        {
            "comment": "This code performs multi-head self-attention with various masks for the transformer layer, taking into account the maximum allowed distance between query and key, applying positional bias if needed, scaling by the head_scale parameters, and incorporating a residual term suggested in a recent paper. The output is determined based on the provided inputs 'q', 'k', and 'v'.",
            "location": "\"/media/root/Toshiba XG3/works/Gemini/docs/src/gemini_torch/transformer.py\":964-998",
            "content": "            masks.append(~attn_mask)\n        if exists(self.max_attend_past):\n            range_q = torch.arange(j - i, j, device=device)\n            range_k = torch.arange(j, device=device)\n            dist = rearrange(range_q, \"i -> 1 1 i 1\") - rearrange(\n                range_k, \"j -> 1 1 1 j\"\n            )\n            max_attend_past_mask = dist > self.max_attend_past\n            masks.append(max_attend_past_mask)\n        if len(masks) > 0:\n            final_attn_mask = ~or_reduce(masks)\n        # prepare relative positional bias, if needed\n        attn_bias = None\n        if exists(rel_pos):\n            attn_bias = rel_pos(i, j)\n        # attention is all we need\n        out, intermediates = self.attend(\n            q, k, v, mask=final_attn_mask, attn_bias=attn_bias, prev_attn=prev_attn\n        )\n        # https://arxiv.org/abs/2208.06061 proposes to add a residual for better gradients\n        if exists(r):\n            out = out * r + out\n        # normformer scaling of heads\n        if head_scale:\n            out = out * self.head_scale_params"
        },
        {
            "comment": "The code is defining a class called `AttentionLayers` that extends `nn.Module`. This class represents a transformer attention layer, and it includes various parameters for configuring the layer's behavior (e.g., number of heads, causality, cross-attention, etc.). The code also defines a function named `to_out` which combines the layer's attention heads and applies optional gating before combining the heads. Additionally, the function includes an optional mask to prevent certain outputs from contributing to the final output. Finally, the class includes several other parameters that control additional functionalities such as normalization and dynamic positional bias.",
            "location": "\"/media/root/Toshiba XG3/works/Gemini/docs/src/gemini_torch/transformer.py\":1000-1041",
            "content": "        # merge heads\n        out = rearrange(out, \"b h n d -> b n (h d)\")\n        # alphafold2 styled gating of the values\n        if exists(self.to_v_gate):\n            gates = self.to_v_gate(x)\n            out = out * gates.sigmoid()\n        # combine the heads\n        out = self.to_out(out)\n        if exists(mask):\n            mask = rearrange(mask, \"b n -> b n 1\")\n            out = out.masked_fill(~mask, 0.0)\n        return out, intermediates\nclass AttentionLayers(nn.Module):\n    def __init__(\n        self,\n        dim,\n        depth,\n        heads=8,\n        causal=False,\n        cross_attend=False,\n        only_cross=False,\n        use_scalenorm=False,\n        use_rmsnorm=False,\n        use_simple_rmsnorm=False,\n        alibi_pos_bias=False,\n        alibi_num_heads=None,\n        rel_pos_bias=False,\n        rel_pos_num_buckets=32,\n        rel_pos_max_distance=128,\n        dynamic_pos_bias=False,\n        dynamic_pos_bias_log_distance=False,\n        dynamic_pos_bias_mlp_depth=2,\n        dynamic_pos_bias_norm=False,"
        },
        {
            "comment": "This code is initializing a Transformer layer in PyTorch. The function takes several parameters including rotary position embeddings, custom layers, and various other configurations. It uses the super() method to call its parent class constructor and sets some variables based on given arguments. The function also separates other key-value pairs into 'ff_' and 'attn_' groups for later usage.",
            "location": "\"/media/root/Toshiba XG3/works/Gemini/docs/src/gemini_torch/transformer.py\":1042-1075",
            "content": "        rotary_pos_emb=False,\n        rotary_emb_dim=None,\n        rotary_xpos=False,\n        rotary_interpolation_factor=1.0,\n        rotary_xpos_scale_base=512,\n        rotary_base_rescale_factor=1.0,\n        custom_layers=None,\n        sandwich_coef=None,\n        par_ratio=None,\n        residual_attn=False,\n        cross_residual_attn=False,\n        macaron=False,\n        pre_norm=True,\n        pre_norm_has_final_norm=True,\n        gate_residual=False,\n        scale_residual=False,\n        scale_residual_constant=1.0,\n        deepnorm=False,\n        shift_tokens=0,\n        sandwich_norm=False,\n        resi_dual=False,\n        resi_dual_scale=1.0,\n        zero_init_branch_output=False,\n        layer_dropout=0.0,\n        cross_attn_tokens_dropout=0.0,\n        **kwargs,\n    ):\n        super().__init__()\n        rotary_pos_emb = rotary_pos_emb or rotary_xpos\n        ff_kwargs, kwargs = groupby_prefix_and_trim(\"ff_\", kwargs)\n        attn_kwargs, kwargs = groupby_prefix_and_trim(\"attn_\", kwargs)\n        dim_head = attn_kwargs.get(\"dim_head\", DEFAULT_DIM_HEAD)"
        },
        {
            "comment": "This code initializes a Transformer model by setting its dimensions and depth, creating a list of layers, and handling positional embeddings. It asserts that only one type of positional bias can be chosen and checks for compatibility between rotary xpos and causal flags.",
            "location": "\"/media/root/Toshiba XG3/works/Gemini/docs/src/gemini_torch/transformer.py\":1077-1105",
            "content": "        self.dim = dim\n        self.depth = depth\n        self.layers = nn.ModuleList([])\n        self.has_pos_emb = rel_pos_bias or rotary_pos_emb\n        rotary_emb_dim = max(default(rotary_emb_dim, dim_head // 2), 32)\n        assert not (\n            rotary_xpos and not causal\n        ), \"rotary xpos is not compatible with bidirectional attention\"\n        self.rotary_pos_emb = (\n            RotaryEmbedding(\n                rotary_emb_dim,\n                use_xpos=rotary_xpos,\n                scale_base=rotary_xpos_scale_base,\n                interpolation_factor=rotary_interpolation_factor,\n                base_rescale_factor=rotary_base_rescale_factor,\n            )\n            if rotary_pos_emb\n            else None\n        )\n        assert not (\n            alibi_pos_bias and rel_pos_bias\n        ), \"you can only choose Alibi positional bias or T5 relative positional bias, not both\"\n        assert (\n            rel_pos_num_buckets <= rel_pos_max_distance\n        ), \"number of relative position buckets must be less than the relative position max distance\""
        },
        {
            "comment": "Code adds position bias to attention scores. It checks if relative, dynamic, or T5 positional bias is chosen and assigns the corresponding RelativePositionBias or DynamicPositionBias object. Flash attention is not compatible with position biases.",
            "location": "\"/media/root/Toshiba XG3/works/Gemini/docs/src/gemini_torch/transformer.py\":1107-1133",
            "content": "        # relative positional bias\n        flash_attn = attn_kwargs.get(\"flash\", False)\n        assert (\n            int(rel_pos_bias) + int(dynamic_pos_bias) + int(alibi_pos_bias)\n        ) <= 1, \"you can only choose up to one of t5, alibi, or dynamic positional bias\"\n        self.rel_pos = None\n        if rel_pos_bias:\n            assert (\n                not flash_attn\n            ), \"flash attention not compatible with t5 relative positional bias\"\n            self.rel_pos = RelativePositionBias(\n                scale=dim_head**0.5,\n                causal=causal,\n                heads=heads,\n                num_buckets=rel_pos_num_buckets,\n                max_distance=rel_pos_max_distance,\n            )\n        elif dynamic_pos_bias:\n            assert (\n                not flash_attn\n            ), \"flash attention not compatible with dynamic positional bias\"\n            self.rel_pos = DynamicPositionBias(\n                dim=dim // 4,\n                heads=heads,\n                log_distance=dynamic_pos_bias_log_distance,"
        },
        {
            "comment": "This code checks the configuration and initializes the transformer model components. It handles the number of heads for dynamic positional bias, alibi positional bias, deepnorm settings, pre-norm, sandwich norm, and residual scale. It ensures compatibility and proper initialization to create the final transformer architecture.",
            "location": "\"/media/root/Toshiba XG3/works/Gemini/docs/src/gemini_torch/transformer.py\":1134-1159",
            "content": "                depth=dynamic_pos_bias_mlp_depth,\n                norm=dynamic_pos_bias_norm,\n            )\n        elif alibi_pos_bias:\n            alibi_num_heads = default(alibi_num_heads, heads)\n            assert (\n                alibi_num_heads <= heads\n            ), \"number of ALiBi heads must be less than the total number of heads\"\n            self.rel_pos = AlibiPositionalBias(heads=alibi_num_heads, total_heads=heads)\n        # determine deepnorm and residual scale\n        if deepnorm:\n            assert (\n                scale_residual_constant == 1\n            ), \"scale residual constant is being overridden by deep norm settings\"\n            pre_norm = sandwich_norm = resi_dual = False\n            scale_residual = True\n            scale_residual_constant = (2 * depth) ** 0.25\n        assert (\n            int(sandwich_norm) + int(resi_dual)\n        ) <= 1, \"either sandwich norm or resiDual is selected, but not both\"\n        assert not (\n            not pre_norm and sandwich_norm\n        ), \"sandwich norm cannot be used when not using prenorm\""
        },
        {
            "comment": "The code is initializing the object's attributes based on input flags. It checks for valid conditions, such as ensuring that only one type of normalization method can be used and that flash attention is not combined with residual attention. The code then assigns the correct subclass of a norm layer based on the provided flag.",
            "location": "\"/media/root/Toshiba XG3/works/Gemini/docs/src/gemini_torch/transformer.py\":1161-1190",
            "content": "        if resi_dual:\n            pre_norm = False\n        self.pre_norm = pre_norm\n        self.sandwich_norm = sandwich_norm\n        self.resi_dual = resi_dual\n        assert (\n            0 < resi_dual_scale <= 1.0\n        ), \"resiDual prenorm residual must be scaled by a factor greater than 0 and less than or equal to 1.\"\n        self.resi_dual_scale = resi_dual_scale\n        self.residual_attn = residual_attn\n        self.cross_residual_attn = cross_residual_attn\n        assert not (\n            flash_attn and (residual_attn or cross_residual_attn)\n        ), \"flash attention is not compatible with residual attention\"\n        self.cross_attend = cross_attend\n        assert (\n            int(use_scalenorm) + int(use_rmsnorm) + int(use_simple_rmsnorm)\n        ) <= 1, \"you can only use either scalenorm, rmsnorm, or simple rmsnorm\"\n        if use_scalenorm:\n            norm_class = ScaleNorm\n        elif use_rmsnorm:\n            norm_class = RMSNorm\n        elif use_simple_rmsnorm:\n            norm_class = SimpleRMSNorm"
        },
        {
            "comment": "This code sets up the layer order and parameters for a transformer model based on input arguments. It creates a LayerNorm function, determines the default block order based on cross-attention flags, adjusts the block order if macaron is set, and handles zero initialization for output if specified. If custom_layers are provided, it uses them instead of the default block order. If par_ratio is given, it calculates a partial attention layer count based on the ratio and adjusts the block order accordingly. The code also calculates a depth cutoff for the attention layers based on the PAR paper's suggestion.",
            "location": "\"/media/root/Toshiba XG3/works/Gemini/docs/src/gemini_torch/transformer.py\":1191-1223",
            "content": "        else:\n            norm_class = nn.LayerNorm\n        norm_fn = partial(norm_class, dim)\n        if cross_attend and not only_cross:\n            default_block = (\"a\", \"c\", \"f\")\n        elif cross_attend and only_cross:\n            default_block = (\"c\", \"f\")\n        else:\n            default_block = (\"a\", \"f\")\n        if macaron:\n            default_block = (\"f\",) + default_block\n        # zero init\n        if zero_init_branch_output:\n            attn_kwargs = {**attn_kwargs, \"zero_init_output\": True}\n            ff_kwargs = {**ff_kwargs, \"zero_init_output\": True}\n        # calculate layer block order\n        if exists(custom_layers):\n            layer_types = custom_layers\n        elif exists(par_ratio):\n            par_depth = depth * len(default_block)\n            assert 1 < par_ratio <= par_depth, \"par ratio out of range\"\n            default_block = tuple(filter(not_equals(\"f\"), default_block))\n            par_attn = par_depth // par_ratio\n            depth_cut = (\n                par_depth * 2 // 3\n            )  # 2 / 3 attention layer cutoff suggested by PAR paper"
        },
        {
            "comment": "This code defines the layer types for a transformer model. It uses default block, par_ratio, and sandwich coefficient to determine the structure of each layer type. If no specific configuration is given, it defaults to using the default block repeated depth times. The number of attention layers and layer dropouts are also calculated based on these configurations.",
            "location": "\"/media/root/Toshiba XG3/works/Gemini/docs/src/gemini_torch/transformer.py\":1224-1248",
            "content": "            par_width = (depth_cut + depth_cut // par_attn) // par_attn\n            assert (\n                len(default_block) <= par_width\n            ), \"default block is too large for par_ratio\"\n            par_block = default_block + (\"f\",) * (par_width - len(default_block))\n            par_head = par_block * par_attn\n            layer_types = par_head + (\"f\",) * (par_depth - len(par_head))\n        elif exists(sandwich_coef):\n            assert (\n                sandwich_coef > 0 and sandwich_coef <= depth\n            ), \"sandwich coefficient should be less than the depth\"\n            layer_types = (\n                (\"a\",) * sandwich_coef\n                + default_block * (depth - sandwich_coef)\n                + (\"f\",) * sandwich_coef\n            )\n        else:\n            layer_types = default_block * depth\n        self.layer_types = layer_types\n        self.num_attn_layers = len(list(filter(equals(\"a\"), layer_types)))\n        # stochastic depth\n        self.layer_dropouts = cast_tuple(layer_dropout, len(layer_types))"
        },
        {
            "comment": "The code initializes a Transformer model with structured dropout for cross attending, token shifting, and post-norm layers. It iterates through the layer types (a, c, f) to construct Attention or FeedForward layers. If the layer type is not recognized, it raises an exception. Token shifting is applied if greater than 0. The final_norm is either a normalization function or identity depending on pre_norm and resi_dual flags.",
            "location": "\"/media/root/Toshiba XG3/works/Gemini/docs/src/gemini_torch/transformer.py\":1250-1279",
            "content": "        # structured dropout for cross attending\n        self.cross_attn_tokens_dropout = cross_attn_tokens_dropout\n        # calculate token shifting\n        shift_tokens = cast_tuple(shift_tokens, len(layer_types))\n        # whether it has post norm\n        self.final_norm = norm_fn() if pre_norm or resi_dual else nn.Identity()\n        # iterate and construct layers\n        for ind, (layer_type, layer_shift_tokens) in enumerate(\n            zip(self.layer_types, shift_tokens)\n        ):\n            ind == (len(self.layer_types) - 1)\n            if layer_type == \"a\":\n                layer = Attention(dim, heads=heads, causal=causal, **attn_kwargs)\n            elif layer_type == \"c\":\n                layer = Attention(dim, heads=heads, **attn_kwargs)\n            elif layer_type == \"f\":\n                layer = FeedForward(dim, **ff_kwargs)\n                layer = layer if not macaron else Scale(0.5, layer)\n            else:\n                raise Exception(f\"invalid layer type {layer_type}\")\n            if layer_shift_tokens > 0:"
        },
        {
            "comment": "This code defines a transformer model with configurable parameters such as pre-norm, sandwich norm, residual connection scale, and deepnorm initialization. It initializes the layers and applies normalization, token shifting, and residuals.",
            "location": "\"/media/root/Toshiba XG3/works/Gemini/docs/src/gemini_torch/transformer.py\":1280-1308",
            "content": "                shift_range_upper = layer_shift_tokens + 1\n                shift_range_lower = -layer_shift_tokens if not causal else 0\n                layer = ShiftTokens(range(shift_range_lower, shift_range_upper), layer)\n            residual_fn = GRUGating if gate_residual else Residual\n            residual = residual_fn(\n                dim,\n                scale_residual=scale_residual,\n                scale_residual_constant=scale_residual_constant,\n            )\n            pre_branch_norm = norm_fn() if pre_norm else None\n            post_branch_norm = norm_fn() if sandwich_norm else None\n            post_main_norm = norm_fn() if not pre_norm else None\n            norms = nn.ModuleList([pre_branch_norm, post_branch_norm, post_main_norm])\n            self.layers.append(nn.ModuleList([norms, layer, residual]))\n        if deepnorm:\n            init_gain = (8 * depth) ** -0.25\n            deepnorm_init(self, init_gain)\n    def forward(\n        self,\n        x,\n        context=None,\n        mask=None,\n        context_mask=None,"
        },
        {
            "comment": "This code defines a transformer layer in the Gemini framework, where it checks if context is passed when cross_attend flag is set. It initializes empty lists for hiddens, layer_hiddens, and intermediates. It copies mems, sets prev_attn and prev_cross_attn to None, and prepares rotary_pos_emb. Then it performs a residual connection on the input x with resi_dual_scale. Finally, it iterates over layers, checking if layer_dropout is non-zero during training and drops the layer with a certain probability.",
            "location": "\"/media/root/Toshiba XG3/works/Gemini/docs/src/gemini_torch/transformer.py\":1309-1341",
            "content": "        attn_mask=None,\n        self_attn_context_mask=None,\n        mems=None,\n        return_hiddens=False,\n    ):\n        assert not (\n            self.cross_attend ^ exists(context)\n        ), \"context must be passed in if cross_attend is set to True\"\n        hiddens = []\n        layer_hiddens = []\n        intermediates = []\n        prev_attn = None\n        prev_cross_attn = None\n        mems = mems.copy() if exists(mems) else [None] * self.num_attn_layers\n        rotary_pos_emb = None\n        if exists(self.rotary_pos_emb):\n            max_rotary_emb_length = max(\n                list(map(lambda m: (m.shape[1] if exists(m) else 0) + x.shape[1], mems))\n            )\n            rotary_pos_emb = self.rotary_pos_emb(max_rotary_emb_length, x.device)\n        outer_residual = x * self.resi_dual_scale\n        for ind, (layer_type, (norm, block, residual_fn), layer_dropout) in enumerate(\n            zip(self.layer_types, self.layers, self.layer_dropouts)\n        ):\n            ind == (len(self.layers) - 1)\n            if self.training and layer_dropout > 0.0 and random() < layer_dropout:"
        },
        {
            "comment": "This code performs a series of transformer layer operations, such as handling different layer types (\"a\" and \"c\"), applying dropout to the context sequence, and optionally storing hidden states. It also includes layer-specific normalization steps before passing the input through a block function.",
            "location": "\"/media/root/Toshiba XG3/works/Gemini/docs/src/gemini_torch/transformer.py\":1342-1373",
            "content": "                continue\n            if layer_type == \"a\":\n                if return_hiddens:\n                    hiddens.append(x)\n                layer_mem = mems.pop(0) if mems else None\n            if layer_type == \"c\":\n                if self.training and self.cross_attn_tokens_dropout > 0.0:\n                    context, context_mask = dropout_seq(\n                        context, context_mask, self.cross_attn_tokens_dropout\n                    )\n            inner_residual = x\n            if return_hiddens:\n                layer_hiddens.append(x)\n            pre_norm, post_branch_norm, post_main_norm = norm\n            if exists(pre_norm):\n                x = pre_norm(x)\n            if layer_type == \"a\":\n                out, inter = block(\n                    x,\n                    mask=mask,\n                    context_mask=self_attn_context_mask,\n                    attn_mask=attn_mask,\n                    rel_pos=self.rel_pos,\n                    rotary_pos_emb=rotary_pos_emb,\n                    prev_attn=prev_attn,"
        },
        {
            "comment": "This code handles different layer types in a transformer model. It applies blocks for attention, feedforward networks, and optional residual connections. Intermediates are stored if required, and post-layer normalization is applied if present. Previous attention values are updated accordingly for specific layer types.",
            "location": "\"/media/root/Toshiba XG3/works/Gemini/docs/src/gemini_torch/transformer.py\":1374-1404",
            "content": "                    mem=layer_mem,\n                )\n            elif layer_type == \"c\":\n                out, inter = block(\n                    x,\n                    context=context,\n                    mask=mask,\n                    context_mask=context_mask,\n                    prev_attn=prev_cross_attn,\n                )\n            elif layer_type == \"f\":\n                out = block(x)\n            if self.resi_dual:\n                outer_residual = outer_residual + out * self.resi_dual_scale\n            if exists(post_branch_norm):\n                out = post_branch_norm(out)\n            x = residual_fn(out, inner_residual)\n            if layer_type in (\"a\", \"c\") and return_hiddens:\n                intermediates.append(inter)\n            if layer_type == \"a\" and self.residual_attn:\n                prev_attn = inter.pre_softmax_attn\n            elif layer_type == \"c\" and self.cross_residual_attn:\n                prev_cross_attn = inter.pre_softmax_attn\n            if exists(post_main_norm):\n                x = post_main_norm(x)"
        },
        {
            "comment": "The code defines classes for Encoder, Decoder, CrossAttender, and ViTransformerWrapper. Encoders and Decoders are initialized with causal set to False and True respectively. CrossAttender is initialized with cross_attend and only_cross set to True. ViTransformerWrapper is a nn.Module that wraps these classes for use in vision transformers.",
            "location": "\"/media/root/Toshiba XG3/works/Gemini/docs/src/gemini_torch/transformer.py\":1406-1444",
            "content": "        if return_hiddens:\n            layer_hiddens.append(x)\n        if self.resi_dual:\n            x = x + self.final_norm(outer_residual)\n        else:\n            x = self.final_norm(x)\n        if return_hiddens:\n            intermediates = LayerIntermediates(\n                hiddens=hiddens,\n                attn_intermediates=intermediates,\n                layer_hiddens=layer_hiddens,\n            )\n            return x, intermediates\n        return x\nclass Encoder(AttentionLayers):\n    def __init__(self, **kwargs):\n        assert \"causal\" not in kwargs, \"cannot set causality on encoder\"\n        super().__init__(causal=False, **kwargs)\nclass Decoder(AttentionLayers):\n    def __init__(self, **kwargs):\n        assert \"causal\" not in kwargs, \"cannot set causality on decoder\"\n        super().__init__(causal=True, **kwargs)\nclass CrossAttender(AttentionLayers):\n    def __init__(self, **kwargs):\n        super().__init__(cross_attend=True, only_cross=True, **kwargs)\nclass ViTransformerWrapper(nn.Module):\n    def __init__("
        },
        {
            "comment": "This code is initializing a Transformer model with specified parameters. It checks the input arguments, creates necessary layers like position embedding, patch to embedding, and attention layers. Additionally, it handles dropout and normalization based on provided settings.",
            "location": "\"/media/root/Toshiba XG3/works/Gemini/docs/src/gemini_torch/transformer.py\":1445-1478",
            "content": "        self,\n        *,\n        image_size,\n        patch_size,\n        attn_layers,\n        channels=3,\n        num_classes=None,\n        post_emb_norm=False,\n        emb_dropout=0.0,\n    ):\n        super().__init__()\n        assert isinstance(attn_layers, Encoder), \"attention layers must be an Encoder\"\n        assert divisible_by(\n            image_size, patch_size\n        ), \"image dimensions must be divisible by the patch size\"\n        dim = attn_layers.dim\n        num_patches = (image_size // patch_size) ** 2\n        patch_dim = channels * patch_size**2\n        self.patch_size = patch_size\n        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches, dim))\n        self.patch_to_embedding = nn.Sequential(\n            nn.LayerNorm(patch_dim), nn.Linear(patch_dim, dim), nn.LayerNorm(dim)\n        )\n        self.post_emb_norm = nn.LayerNorm(dim) if post_emb_norm else nn.Identity()\n        self.dropout = nn.Dropout(emb_dropout)\n        self.attn_layers = attn_layers\n        self.mlp_head = (\n            nn.Linear(dim, num_classes) if exists(num_classes) else nn.Identity()"
        },
        {
            "comment": "The code defines a Transformer class for image processing tasks. It takes an image and performs patch-based embedding, position embedding, and self-attention layers to generate a feature map. The feature map can be optionally passed through an MLP head for classification or regression tasks. The code also includes various parameters and options for customization such as embedding dimensions, normalization, dropout rates, and type of positional embeddings.",
            "location": "\"/media/root/Toshiba XG3/works/Gemini/docs/src/gemini_torch/transformer.py\":1479-1520",
            "content": "        )\n    def forward(self, img, return_embeddings=False):\n        p = self.patch_size\n        x = rearrange(img, \"b c (h p1) (w p2) -> b (h w) (p1 p2 c)\", p1=p, p2=p)\n        x = self.patch_to_embedding(x)\n        n = x.shape[1]\n        x = x + self.pos_embedding[:, :n]\n        x = self.post_emb_norm(x)\n        x = self.dropout(x)\n        x = self.attn_layers(x)\n        if not exists(self.mlp_head) or return_embeddings:\n            return x\n        x = x.mean(dim=-2)\n        return self.mlp_head(x)\nclass Transformer(nn.Module):\n    def __init__(\n        self,\n        *,\n        num_tokens,\n        max_seq_len,\n        attn_layers,\n        emb_dim=None,\n        max_mem_len=0,\n        shift_mem_down=0,\n        emb_dropout=0.0,\n        post_emb_norm=False,\n        num_memory_tokens=None,\n        tie_embedding=False,\n        logits_dim=None,\n        use_abs_pos_emb=True,\n        scaled_sinu_pos_emb=False,\n        l2norm_embed=False,\n        emb_frac_gradient=1.0,  # GLM-130B and Cogview successfully used this, set at 0.1"
        },
        {
            "comment": "This code initializes a class object with given parameters. It checks the type of attention layers and sets embedding and positional embeddings based on provided arguments, while considering the gradient flow to the embedding layer.",
            "location": "\"/media/root/Toshiba XG3/works/Gemini/docs/src/gemini_torch/transformer.py\":1521-1549",
            "content": "        attn_z_loss_weight=1e-4,\n    ):\n        super().__init__()\n        assert isinstance(\n            attn_layers, AttentionLayers\n        ), \"attention layers must be one of Encoder or Decoder\"\n        dim = attn_layers.dim\n        emb_dim = default(emb_dim, dim)\n        self.emb_dim = emb_dim\n        self.num_tokens = num_tokens\n        self.max_seq_len = max_seq_len\n        self.max_mem_len = max_mem_len\n        self.shift_mem_down = shift_mem_down\n        self.l2norm_embed = l2norm_embed\n        self.token_emb = TokenEmbedding(emb_dim, num_tokens, l2norm_embed=l2norm_embed)\n        if not (use_abs_pos_emb and not attn_layers.has_pos_emb):\n            self.pos_emb = always(0)\n        elif scaled_sinu_pos_emb:\n            self.pos_emb = ScaledSinusoidalEmbedding(emb_dim)\n        else:\n            self.pos_emb = AbsolutePositionalEmbedding(\n                emb_dim, max_seq_len, l2norm_embed=l2norm_embed\n            )\n        self.emb_frac_gradient = emb_frac_gradient  # fraction of the gradient that should go to the embedding, https://arxiv.org/abs/2105.13290"
        },
        {
            "comment": "The code creates a Transformer model with optional embedding normalization, dropout, and projection. It also includes a linear layer to convert embeddings to logits and supports memory tokens for Memory Transformers paper. The `init_` function initializes the weights of the token and positional embeddings if l2norm_embed is True.",
            "location": "\"/media/root/Toshiba XG3/works/Gemini/docs/src/gemini_torch/transformer.py\":1551-1576",
            "content": "        self.post_emb_norm = nn.LayerNorm(emb_dim) if post_emb_norm else nn.Identity()\n        self.emb_dropout = nn.Dropout(emb_dropout)\n        self.project_emb = nn.Linear(emb_dim, dim) if emb_dim != dim else nn.Identity()\n        self.attn_layers = attn_layers\n        self.init_()\n        logits_dim = default(logits_dim, num_tokens)\n        self.to_logits = (\n            nn.Linear(dim, logits_dim)\n            if not tie_embedding\n            else lambda t: t @ self.token_emb.emb.weight.t()\n        )\n        # memory tokens (like [cls]) from Memory Transformers paper\n        num_memory_tokens = default(num_memory_tokens, 0)\n        self.num_memory_tokens = num_memory_tokens\n        if num_memory_tokens > 0:\n            self.memory_tokens = nn.Parameter(torch.randn(num_memory_tokens, dim))\n    def init_(self):\n        if self.l2norm_embed:\n            nn.init.normal_(self.token_emb.emb.weight, std=1e-5)\n            if not isinstance(self.pos_emb, always):\n                nn.init.normal_(self.pos_emb.emb.weight, std=1e-5)"
        },
        {
            "comment": "This code defines a function for a transformer model's forward pass. It takes input tensor `x`, performs token embedding and positional embedding, and adds them together. The function also supports returning intermediate outputs (mems, attn) and an optional attention loss term. The function uses device-specific operations, checks if external position embeddings are used, and handles summing external embeddings for self-conditioning in non-autoregressive training.",
            "location": "\"/media/root/Toshiba XG3/works/Gemini/docs/src/gemini_torch/transformer.py\":1577-1614",
            "content": "            return\n        nn.init.kaiming_normal_(self.token_emb.emb.weight)\n    def forward(\n        self,\n        x,\n        return_embeddings=False,\n        return_logits_and_embeddings=False,\n        return_intermediates=False,\n        mask=None,\n        return_mems=False,\n        return_attn=False,\n        mems=None,\n        pos=None,\n        prepend_embeds=None,\n        sum_embeds=None,\n        return_attn_z_loss=False,\n        attn_z_loss_weight=1e-4,\n        **kwargs,\n    ):\n        b, n, device, num_mem, emb_frac_gradient = (\n            *x.shape,\n            x.device,\n            self.num_memory_tokens,\n            self.emb_frac_gradient,\n        )\n        return_hiddens = (\n            return_mems | return_attn | return_intermediates | return_attn_z_loss\n        )\n        # absolute positional embedding\n        external_pos_emb = exists(pos) and pos.dtype != torch.long\n        pos_emb = self.pos_emb(x, pos=pos) if not external_pos_emb else pos\n        x = self.token_emb(x) + pos_emb\n        # for summing embeddings passed externally - needs this for self-conditioning in non-autoregressive training"
        },
        {
            "comment": "This code applies post-embedding normalization, prepends image embeddings if needed, reduces embedding gradient flow, applies embedding dropout, and projects the embeddings. It also optionally concatenates memory tokens for a specified number of memories.",
            "location": "\"/media/root/Toshiba XG3/works/Gemini/docs/src/gemini_torch/transformer.py\":1616-1647",
            "content": "        if exists(sum_embeds):\n            x = x + sum_embeds\n        # post embedding norm, purportedly leads to greater stabilization\n        x = self.post_emb_norm(x)\n        # whether to append embeds, as in PaLI, for image embeddings\n        if exists(prepend_embeds):\n            prepend_seq, prepend_dim = prepend_embeds.shape[1:]\n            assert (\n                prepend_dim == x.shape[-1]\n            ), \"prepended embeddings need to have same dimensions as text model dimensions\"\n            x = torch.cat((prepend_embeds, x), dim=-2)\n        # whether to reduce the gradient going to the embedding, from cogview paper, corroborated by GLM-130B model\n        if emb_frac_gradient < 1:\n            assert emb_frac_gradient > 0\n            x = x * emb_frac_gradient + x.detach() * (1 - emb_frac_gradient)\n        # embedding dropout\n        x = self.emb_dropout(x)\n        x = self.project_emb(x)\n        if num_mem > 0:\n            mem = repeat(self.memory_tokens, \"n d -> b n d\", b=b)\n            x = torch.cat((mem, x), dim=1)"
        },
        {
            "comment": "This code appears to be part of a Transformer model implementation, specifically handling masking and returning various outputs based on the function parameters. It handles masking for attention, shifts memory tokens down if required, applies the attention layers, and generates different output types such as logits, embeddings, or both, depending on the return arguments. Additionally, it calculates an attention loss term called 'attn_z_loss' based on pre-softmax attention values.",
            "location": "\"/media/root/Toshiba XG3/works/Gemini/docs/src/gemini_torch/transformer.py\":1649-1677",
            "content": "            # auto-handle masking after appending memory tokens\n            if exists(mask):\n                mask = pad_at_dim(mask, (num_mem, 0), dim=-1, value=True)\n        if self.shift_mem_down and exists(mems):\n            mems_l, mems_r = mems[: self.shift_mem_down], mems[self.shift_mem_down :]\n            mems = [*mems_r, *mems_l]\n        if return_hiddens:\n            x, intermediates = self.attn_layers(\n                x, mask=mask, mems=mems, return_hiddens=True, **kwargs\n            )\n        else:\n            x = self.attn_layers(x, mask=mask, mems=mems, **kwargs)\n        mem, x = x[:, :num_mem], x[:, num_mem:]\n        if return_logits_and_embeddings:\n            out = (self.to_logits(x), x)\n        elif return_embeddings:\n            out = x\n        else:\n            out = self.to_logits(x)\n        if return_attn_z_loss:\n            pre_softmax_attns = list(\n                map(lambda t: t.pre_softmax_attn, intermediates.attn_intermediates)\n            )\n            intermediates.attn_z_loss = calc_z_loss("
        },
        {
            "comment": "This code defines a function that processes the output of an attention mechanism in a transformer model. It accepts options to return different intermediate outputs, such as the pre-softmax attention weights, memory states, or attention maps. If no option is specified, it only returns the final output.",
            "location": "\"/media/root/Toshiba XG3/works/Gemini/docs/src/gemini_torch/transformer.py\":1678-1703",
            "content": "                pre_softmax_attns, weight=attn_z_loss_weight\n            )\n            return_intermediates = True\n        if return_intermediates:\n            return out, intermediates\n        if return_mems:\n            hiddens = intermediates.hiddens\n            new_mems = (\n                list(map(lambda pair: torch.cat(pair, dim=-2), zip(mems, hiddens)))\n                if exists(mems)\n                else hiddens\n            )\n            new_mems = list(\n                map(lambda t: t[..., -self.max_mem_len :, :].detach(), new_mems)\n            )\n            return out, new_mems\n        if return_attn:\n            attn_maps = list(\n                map(lambda t: t.post_softmax_attn, intermediates.attn_intermediates)\n            )\n            return out, attn_maps\n        return out"
        }
    ]
}